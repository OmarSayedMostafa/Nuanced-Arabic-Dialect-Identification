{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: (20974, 8) 20974\n",
      "Test set: (4997, 8) 4997\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import gensim\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_recall_curve, precision_score, recall_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "\n",
    "#text is already cleaned.\n",
    "#assign cleaned data to these variables.\n",
    "train_data_path = 'cleaned_data/cleaned_train_data_for_subtask1.csv'\n",
    "test_data_path = 'cleaned_data/cleaned_test_data_for_subtask1.csv'\n",
    "#read files.\n",
    "train_data = pd.read_csv(train_data_path)\n",
    "test_data = pd.read_csv(test_data_path)\n",
    "\n",
    "print(\"Train set:\"% train_data.columns, train_data.shape, len(train_data)) \n",
    "print(\"Test set:\"% test_data.columns, test_data.shape, len(test_data)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare train and test data.\n",
    "X_train = train_data['#2_tweet_clean_V2']\n",
    "y_train = train_data['#classes_id']\n",
    "X_test = test_data['#2_tweet_clean_V2']\n",
    "y_test = test_data['#classes_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 48685 unique tokens.\n",
      "Found 17156 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "max_no = 50000\n",
    "sql_len = 250\n",
    "dim = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_no, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=False)\n",
    "tokenizer.fit_on_texts(X_train.values)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_no, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=False)\n",
    "tokenizer.fit_on_texts(X_test.values)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (20974, 250)\n",
      "Shape of data tensor: (4997, 250)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "X_tr = tokenizer.texts_to_sequences(X_train.values)\n",
    "X_tr = pad_sequences(X_tr, maxlen=sql_len)\n",
    "print('Shape of data tensor:', X_tr.shape)\n",
    "\n",
    "X_te = tokenizer.texts_to_sequences(X_test.values)\n",
    "X_te = pad_sequences(X_te, maxlen=sql_len)\n",
    "print('Shape of data tensor:', X_te.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of label tensor: (20974, 21)\n",
      "Shape of label tensor: (4997, 21)\n"
     ]
    }
   ],
   "source": [
    "Y_tr = pd.get_dummies(y_train).values\n",
    "print('Shape of label tensor:', Y_tr.shape)\n",
    "\n",
    "Y_te = pd.get_dummies(y_test).values\n",
    "print('Shape of label tensor:', Y_te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "  Downloading https://files.pythonhosted.org/packages/44/e1/dc0757b20b56c980b5553c1b5c4c32d378c7055ab7bfa92006801ad359ab/Keras-2.4.3-py2.py3-none-any.whl\n",
      "Requirement already satisfied: pyyaml in c:\\users\\maryam\\anaconda3\\lib\\site-packages (from keras) (5.3.1)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\maryam\\anaconda3\\lib\\site-packages (from keras) (1.19.3)\n",
      "Requirement already satisfied: h5py in c:\\users\\maryam\\anaconda3\\lib\\site-packages (from keras) (2.10.0)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\users\\maryam\\anaconda3\\lib\\site-packages (from keras) (1.5.4)\n",
      "Requirement already satisfied: six in c:\\users\\maryam\\anaconda3\\lib\\site-packages (from h5py->keras) (1.15.0)\n",
      "Installing collected packages: keras"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 19.0, however version 20.3.3 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully installed keras-2.4.3\n"
     ]
    }
   ],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "295/295 [==============================] - 125s 416ms/step - loss: 2.6687 - accuracy: 0.2068 - val_loss: 2.4643 - val_accuracy: 0.2712\n",
      "Epoch 2/20\n",
      "295/295 [==============================] - 159s 539ms/step - loss: 2.2957 - accuracy: 0.3258 - val_loss: 2.2987 - val_accuracy: 0.3236\n",
      "Epoch 3/20\n",
      "295/295 [==============================] - 160s 543ms/step - loss: 1.9144 - accuracy: 0.4525 - val_loss: 2.3258 - val_accuracy: 0.3370\n",
      "Epoch 4/20\n",
      "295/295 [==============================] - 159s 539ms/step - loss: 1.6093 - accuracy: 0.5360 - val_loss: 2.4196 - val_accuracy: 0.3456\n",
      "Epoch 5/20\n",
      "295/295 [==============================] - 160s 543ms/step - loss: 1.3512 - accuracy: 0.6080 - val_loss: 2.6188 - val_accuracy: 0.3241\n",
      "Epoch 6/20\n",
      "295/295 [==============================] - 159s 539ms/step - loss: 1.1727 - accuracy: 0.6569 - val_loss: 2.7585 - val_accuracy: 0.3136\n",
      "Epoch 7/20\n",
      "295/295 [==============================] - 159s 540ms/step - loss: 1.0297 - accuracy: 0.6902 - val_loss: 2.8880 - val_accuracy: 0.3189\n",
      "Epoch 8/20\n",
      "295/295 [==============================] - 159s 539ms/step - loss: 0.9270 - accuracy: 0.7231 - val_loss: 3.0889 - val_accuracy: 0.3084\n",
      "Epoch 9/20\n",
      "295/295 [==============================] - 161s 544ms/step - loss: 0.8415 - accuracy: 0.7501 - val_loss: 3.2562 - val_accuracy: 0.2974\n",
      "Epoch 10/20\n",
      "295/295 [==============================] - 336s 1s/step - loss: 0.7825 - accuracy: 0.7636 - val_loss: 3.4103 - val_accuracy: 0.2903\n",
      "Epoch 11/20\n",
      "295/295 [==============================] - 381s 1s/step - loss: 0.7302 - accuracy: 0.7745 - val_loss: 3.5210 - val_accuracy: 0.2846\n",
      "Epoch 12/20\n",
      "295/295 [==============================] - 399s 1s/step - loss: 0.6953 - accuracy: 0.7822 - val_loss: 3.6504 - val_accuracy: 0.2879\n",
      "Epoch 13/20\n",
      "295/295 [==============================] - 377s 1s/step - loss: 0.6749 - accuracy: 0.7898 - val_loss: 3.7741 - val_accuracy: 0.2884\n",
      "Epoch 14/20\n",
      "295/295 [==============================] - 385s 1s/step - loss: 0.6175 - accuracy: 0.8041 - val_loss: 3.8266 - val_accuracy: 0.2836\n",
      "Epoch 15/20\n",
      "295/295 [==============================] - 382s 1s/step - loss: 0.6026 - accuracy: 0.8092 - val_loss: 3.9834 - val_accuracy: 0.2888\n",
      "Epoch 16/20\n",
      "295/295 [==============================] - 382s 1s/step - loss: 0.5964 - accuracy: 0.8128 - val_loss: 4.1444 - val_accuracy: 0.2812\n",
      "Epoch 17/20\n",
      "295/295 [==============================] - 381s 1s/step - loss: 0.5723 - accuracy: 0.8165 - val_loss: 4.1891 - val_accuracy: 0.2831\n",
      "Epoch 18/20\n",
      "295/295 [==============================] - 384s 1s/step - loss: 0.5443 - accuracy: 0.8244 - val_loss: 4.2737 - val_accuracy: 0.2760\n",
      "Epoch 19/20\n",
      "295/295 [==============================] - 589s 2s/step - loss: 0.5286 - accuracy: 0.8287 - val_loss: 4.3765 - val_accuracy: 0.2860\n",
      "Epoch 20/20\n",
      "295/295 [==============================] - 389s 1s/step - loss: 0.5117 - accuracy: 0.8361 - val_loss: 4.4217 - val_accuracy: 0.2836\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow\n",
    "from keras.layers import Embedding, SpatialDropout1D, LSTM, Dense\n",
    "#import tensorsflow_addons as tfa\n",
    "model = keras.Sequential()\n",
    "model.add(Embedding(max_no, dim, input_shape=(sql_len,)))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(21, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 64\n",
    "\n",
    "history = model.fit(X_tr, Y_tr, epochs=epochs, batch_size=batch_size,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14653626397278355\n"
     ]
    }
   ],
   "source": [
    "y_pred= model.predict(X_te)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "y_test = test_data['#classes_id']\n",
    "Y_te = pd.get_dummies(y_test).values\n",
    "\n",
    "Y_te = np.argmax(Y_te, axis=1)\n",
    "print(f1_score(Y_te, y_pred, average='macro'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      precision    recall  f1-score   support\n",
      "\n",
      "               Egypt       0.52      0.60      0.56      1041\n",
      "                Iraq       0.38      0.41      0.39       663\n",
      "        Saudi_Arabia       0.25      0.27      0.26       519\n",
      "          Mauritania       0.20      0.17      0.18        53\n",
      "             Algeria       0.38      0.34      0.36       430\n",
      "               Syria       0.10      0.11      0.11       278\n",
      "                Oman       0.17      0.15      0.16       355\n",
      "             Tunisia       0.14      0.15      0.15       172\n",
      "             Lebanon       0.10      0.09      0.09       157\n",
      "             Morocco       0.08      0.09      0.08       207\n",
      "            Djibouti       0.00      0.00      0.00        27\n",
      "United_Arab_Emirates       0.08      0.08      0.08       157\n",
      "              Kuwait       0.06      0.06      0.06       105\n",
      "               Libya       0.28      0.23      0.25       314\n",
      "             Bahrain       0.00      0.00      0.00        52\n",
      "               Qatar       0.00      0.00      0.00        52\n",
      "               Yemen       0.08      0.06      0.07       105\n",
      "           Palestine       0.09      0.06      0.07       104\n",
      "              Jordan       0.02      0.02      0.02       104\n",
      "             Somalia       0.21      0.08      0.12        49\n",
      "               Sudan       0.08      0.06      0.07        53\n",
      "\n",
      "            accuracy                           0.29      4997\n",
      "           macro avg       0.15      0.14      0.15      4997\n",
      "        weighted avg       0.28      0.29      0.28      4997\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_names = list(train_data[\"#3_country_label\"].unique())\n",
    "print(classification_report(Y_te, y_pred,target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
