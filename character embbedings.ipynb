{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from keras_wc_embd import get_dicts_generator\n",
    "import keras\n",
    "from keras_wc_embd import get_embedding_layer\n",
    "from keras_wc_embd import WordCharEmbd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = \"./cleaned_data/sample_train_task1.csv\"\n",
    "valid_data_path = './cleaned_data/sample_valid_task1.csv'\n",
    "test_data_path = \"./cleaned_data/cleaned_test_data_for_subtask1.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(train_data_path)\n",
    "valid_data = pd.read_csv(valid_data_path)\n",
    "test_data = pd.read_csv(test_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#1_tweetid</th>\n",
       "      <th>#2_tweet</th>\n",
       "      <th>#3_country_label</th>\n",
       "      <th>#2_tweet_clean_V0</th>\n",
       "      <th>#2_tweet_clean_farasaV0</th>\n",
       "      <th>#2_tweet_clean_V1</th>\n",
       "      <th>#2_tweet_clean_V2</th>\n",
       "      <th>#2_tweet_clean_V3</th>\n",
       "      <th>#classes_id</th>\n",
       "      <th>#len_of_2_tweet_clean_V0</th>\n",
       "      <th>cluster_on_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_17392</td>\n",
       "      <td>الحب ما بينشرى ولا بينباع...</td>\n",
       "      <td>Lebanon</td>\n",
       "      <td>الحب ما بينشرى ولا بينباع</td>\n",
       "      <td>ال حب ما ب ينشرى و لا ب ينباع</td>\n",
       "      <td>الحب ما بينشرى ولا بينباع</td>\n",
       "      <td>الحب بينشرى بينباع</td>\n",
       "      <td>الحب بينشرى بينباع</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_15280</td>\n",
       "      <td>يعني كل يوم الصبح اروح عالرئاسه اصبح على دولته...</td>\n",
       "      <td>Jordan</td>\n",
       "      <td>يعني كل يوم الصبح اروح عالرئاسه اصبح على دولته...</td>\n",
       "      <td>يعني كل يوم ال صبح اروح عالرئاسه أصبح على دول ...</td>\n",
       "      <td>يعني كل يوم الصبح اروح عالراسه اصبح على دولته ...</td>\n",
       "      <td>يعني الصبح اروح عالراسه دولته واخذ مصروفي قروش...</td>\n",
       "      <td>يعني الصبح اروح عالراسه دولته واخذ مصروفي قروش</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_7204</td>\n",
       "      <td>تعبت وانا اداوم س١٠</td>\n",
       "      <td>Oman</td>\n",
       "      <td>تعبت وانا اداوم س١٠</td>\n",
       "      <td>تعب ت و أنا اداوم س 10</td>\n",
       "      <td>تعبت وانا اداوم</td>\n",
       "      <td>تعبت وانا اداوم</td>\n",
       "      <td>تعبت وانا اداوم</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_15564</td>\n",
       "      <td>امبارح تلاقينا قعدنا ع حجر برد و حوالينا عريان...</td>\n",
       "      <td>Egypt</td>\n",
       "      <td>امبارح تلاقينا قعدنا ع حجر برد و حوالينا عريان...</td>\n",
       "      <td>امبارح تلاقي نا قعد نا ع حجر برد و حوالي نا عر...</td>\n",
       "      <td>امبارح تلاقينا قعدنا حجر برد حوالينا عريان الش...</td>\n",
       "      <td>امبارح تلاقينا قعدنا حجر برد حوالينا عريان الش...</td>\n",
       "      <td>امبارح تلاقينا قعدنا حجر برد حوالينا عريان الش...</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_15019</td>\n",
       "      <td>النفسية محتاجة شهر في سويسرا  URL</td>\n",
       "      <td>Algeria</td>\n",
       "      <td>النفسية محتاجة شهر في سويسرا</td>\n",
       "      <td>ال نفسي ة محتاج ة شهر في سويسرا</td>\n",
       "      <td>النفسيه محتاجه شهر في سويسرا</td>\n",
       "      <td>النفسيه محتاجه شهر سويسرا</td>\n",
       "      <td>النفسيه محتاجه شهر سويسرا</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    #1_tweetid                                           #2_tweet  \\\n",
       "0  TRAIN_17392                       الحب ما بينشرى ولا بينباع...   \n",
       "1  TRAIN_15280  يعني كل يوم الصبح اروح عالرئاسه اصبح على دولته...   \n",
       "2   TRAIN_7204                                تعبت وانا اداوم س١٠   \n",
       "3  TRAIN_15564  امبارح تلاقينا قعدنا ع حجر برد و حوالينا عريان...   \n",
       "4  TRAIN_15019                 النفسية محتاجة شهر في سويسرا  URL    \n",
       "\n",
       "  #3_country_label                                  #2_tweet_clean_V0  \\\n",
       "0          Lebanon                          الحب ما بينشرى ولا بينباع   \n",
       "1           Jordan  يعني كل يوم الصبح اروح عالرئاسه اصبح على دولته...   \n",
       "2             Oman                                تعبت وانا اداوم س١٠   \n",
       "3            Egypt  امبارح تلاقينا قعدنا ع حجر برد و حوالينا عريان...   \n",
       "4          Algeria                    النفسية محتاجة شهر في سويسرا      \n",
       "\n",
       "                             #2_tweet_clean_farasaV0  \\\n",
       "0                      ال حب ما ب ينشرى و لا ب ينباع   \n",
       "1  يعني كل يوم ال صبح اروح عالرئاسه أصبح على دول ...   \n",
       "2                             تعب ت و أنا اداوم س 10   \n",
       "3  امبارح تلاقي نا قعد نا ع حجر برد و حوالي نا عر...   \n",
       "4                    ال نفسي ة محتاج ة شهر في سويسرا   \n",
       "\n",
       "                                   #2_tweet_clean_V1  \\\n",
       "0                          الحب ما بينشرى ولا بينباع   \n",
       "1  يعني كل يوم الصبح اروح عالراسه اصبح على دولته ...   \n",
       "2                                    تعبت وانا اداوم   \n",
       "3  امبارح تلاقينا قعدنا حجر برد حوالينا عريان الش...   \n",
       "4                       النفسيه محتاجه شهر في سويسرا   \n",
       "\n",
       "                                   #2_tweet_clean_V2  \\\n",
       "0                                 الحب بينشرى بينباع   \n",
       "1  يعني الصبح اروح عالراسه دولته واخذ مصروفي قروش...   \n",
       "2                                    تعبت وانا اداوم   \n",
       "3  امبارح تلاقينا قعدنا حجر برد حوالينا عريان الش...   \n",
       "4                          النفسيه محتاجه شهر سويسرا   \n",
       "\n",
       "                                   #2_tweet_clean_V3  #classes_id  \\\n",
       "0                                 الحب بينشرى بينباع            8   \n",
       "1     يعني الصبح اروح عالراسه دولته واخذ مصروفي قروش           18   \n",
       "2                                    تعبت وانا اداوم            6   \n",
       "3  امبارح تلاقينا قعدنا حجر برد حوالينا عريان الش...            0   \n",
       "4                          النفسيه محتاجه شهر سويسرا            4   \n",
       "\n",
       "   #len_of_2_tweet_clean_V0  cluster_on_length  \n",
       "0                         5                  2  \n",
       "1                        15                  2  \n",
       "2                         4                  0  \n",
       "3                        21                  1  \n",
       "4                         8                  1  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_generator = get_dicts_generator(\n",
    "#     word_min_freq=2,\n",
    "#     char_min_freq=2,\n",
    "#     word_ignore_case=False,\n",
    "#     char_ignore_case=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for tweet in train_data['#2_tweet_clean_V1'].tolist():\n",
    "#     dict_generator(tweet.split(' '))\n",
    "\n",
    "# for tweet in test_data['#2_tweet_clean_V1'].tolist():\n",
    "#     dict_generator(tweet.split(' '))\n",
    "\n",
    "# for tweet in valid_data['#2_tweet_clean_V1'].tolist():\n",
    "#     dict_generator(tweet.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_dict, char_dict, max_word_len = dict_generator(return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs, embd_layer = get_embedding_layer(\n",
    "#     word_dict_len=len(word_dict),\n",
    "#     char_dict_len=len(char_dict),\n",
    "#     max_word_len=max_word_len,\n",
    "#     word_embd_dim=300,\n",
    "#     char_embd_dim=50,\n",
    "#     char_hidden_dim=150,\n",
    "#     char_hidden_layer_type='lstm',\n",
    "# )\n",
    "# model = keras.models.Model(inputs=inputs, outputs=embd_layer)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_tweets =  train_data['#2_tweet_clean_V1'].str.split(' ').tolist()\n",
    "validation_tweets =  valid_data['#2_tweet_clean_V1'].str.split(' ').tolist()\n",
    "test_tweets =  test_data['#2_tweet_clean_V1'].str.split(' ').tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc_embd = WordCharEmbd(\n",
    "    word_min_freq=0,\n",
    "    char_min_freq=0,\n",
    "    word_ignore_case=False,\n",
    "    char_ignore_case=False,\n",
    ")\n",
    "\n",
    "for tweet in training_tweets:\n",
    "    wc_embd.update_dicts(tweet)\n",
    "    \n",
    "for tweet in test_tweets:\n",
    "    wc_embd.update_dicts(tweet)\n",
    "    \n",
    "for tweet in validation_tweets:\n",
    "    wc_embd.update_dicts(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input_Char (InputLayer)         [(None, None, 33)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Input_Word (InputLayer)         [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Embedding_Char_Pre (Embedding)  (None, None, 33, 30) 4050        Input_Char[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Embedding_Word (Embedding)      (None, None, 300)    17067300    Input_Word[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Embedding_Char (TimeDistributed (None, None, 300)    217200      Embedding_Char_Pre[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "Embedding (Concatenate)         (None, None, 600)    0           Embedding_Word[0][0]             \n",
      "                                                                 Embedding_Char[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "LSTM (LSTM)                     (None, 300)          1081200     Embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Softmax (Dense)                 (None, 21)           6321        LSTM[0][0]                       \n",
      "==================================================================================================\n",
      "Total params: 18,376,071\n",
      "Trainable params: 18,376,071\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omar/anaconda3/envs/NLP/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "inputs, embd_layer = wc_embd.get_embedding_layer()\n",
    "lstm_layer = keras.layers.LSTM(units=300, name='LSTM')(embd_layer)\n",
    "softmax_layer = keras.layers.Dense(units=21, activation='softmax', name='Softmax')(lstm_layer)\n",
    "model = keras.models.Model(inputs=inputs, outputs=softmax_layer)\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=keras.losses.sparse_categorical_crossentropy,\n",
    "    metrics=[keras.metrics.sparse_categorical_accuracy],\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "\n",
    "def batch_generator():\n",
    "    while True:\n",
    "        yield wc_embd.get_batch_input(training_tweets), np.asarray([0, 1])\n",
    "\n",
    "model.fit_generator(\n",
    "    generator=batch_generator(),\n",
    "    steps_per_epoch=1000,\n",
    "    epochs=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
