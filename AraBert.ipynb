{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AraBert.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wO0QGfAxOjo0",
        "outputId": "837b91a5-ccc7-43ee-8bd7-1a337494b1cd"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/87/ef312eef26f5cecd8b17ae9654cdd8d1fae1eb6dbd87257d6d73c128a4d0/transformers-4.3.2-py3-none-any.whl (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 19.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 52.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/5b/44baae602e0a30bcc53fbdbc60bd940c15e143d252d658dfdefce736ece5/tokenizers-0.10.1-cp36-cp36m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 57.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=ced69aaa90ef6764f68264625cb31923abd996e1e1124734420c645564232de5\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70xT5-M-LQPX",
        "outputId": "d467084b-4698-4b8d-8306-5dfd85d9ceab"
      },
      "source": [
        "!git clone https://github.com/OmarSayedMostafa/Nuanced-Arabic-Dialect-Identification.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Nuanced-Arabic-Dialect-Identification'...\n",
            "remote: Enumerating objects: 220, done.\u001b[K\n",
            "remote: Counting objects: 100% (220/220), done.\u001b[K\n",
            "remote: Compressing objects: 100% (164/164), done.\u001b[K\n",
            "remote: Total 220 (delta 121), reused 137 (delta 50), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (220/220), 10.38 MiB | 5.17 MiB/s, done.\n",
            "Resolving deltas: 100% (121/121), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBxkcGx0LT-P",
        "outputId": "5d8ef8ef-a0f7-4cb6-caed-1c7ca3c4ecac"
      },
      "source": [
        "cd Nuanced-Arabic-Dialect-Identification/"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Nuanced-Arabic-Dialect-Identification\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qC_hYT3vIqVP"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "import pandas as pd\n",
        "import re\n",
        "import random\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "# optimizer from hugging face transformers\n",
        "from transformers import AdamW"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7qMojB0Ka1R"
      },
      "source": [
        "pd.set_option('display.max_colwidth', -1)\n",
        "\n",
        "train_data_path = './cleaned_data/cleaned_train_data_for_subtask1.csv'\n",
        "test_data_path = './cleaned_data/cleaned_test_data_for_subtask1.csv'\n",
        "\n",
        "train_dataframe = pd.read_csv(train_data_path)\n",
        "test_dataframe = pd.read_csv(test_data_path)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "NVURBYicLxAy",
        "outputId": "9ff5014d-6e8f-4721-920b-a8931ad2fed8"
      },
      "source": [
        "train_dataframe.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>#1_tweetid</th>\n",
              "      <th>#2_tweet</th>\n",
              "      <th>#3_country_label</th>\n",
              "      <th>#2_tweet_clean_V0</th>\n",
              "      <th>#2_tweet_clean_farasaV0</th>\n",
              "      <th>#2_tweet_clean_V1</th>\n",
              "      <th>#2_tweet_clean_V2</th>\n",
              "      <th>#2_tweet_clean_V3</th>\n",
              "      <th>#classes_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>TRAIN_0</td>\n",
              "      <td>حاجة حلوة اكيد</td>\n",
              "      <td>Egypt</td>\n",
              "      <td>حاجة حلوة اكيد</td>\n",
              "      <td>حاج ة حلو ة أكيد</td>\n",
              "      <td>حاجه حلوه اكيد</td>\n",
              "      <td>حاجه حلوه اكيد</td>\n",
              "      <td>حاجه حلوه اكيد</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>TRAIN_1</td>\n",
              "      <td>عم بشتغلوا للشعب الاميركي اما نحن يكذبوا ويغشوا ويسرقوا ويقتلو شعوبهم ويعملوا لصالح اعدائهم</td>\n",
              "      <td>Iraq</td>\n",
              "      <td>عم بشتغلوا للشعب الاميركي اما نحن يكذبوا ويغشوا ويسرقوا ويقتلو شعوبهم ويعملوا لصالح اعدائهم</td>\n",
              "      <td>عم بشتغلوا ل ال شعب ال أميركي اما نحن يكذب وا و يغش وا و يسرق وا و يقتلو شعوب هم و يعمل وا ل صالح اعدائ هم</td>\n",
              "      <td>عم بشتغلوا لشعب الاميركي اما نحن يكذبوا ويغشوا ويسرقوا ويقتلو شعوبهم ويعملوا لصالح اعداهم</td>\n",
              "      <td>عم بشتغلوا لشعب الاميركي يكذبوا ويغشوا ويسرقوا ويقتلو شعوبهم ويعملوا لصالح اعداهم</td>\n",
              "      <td>عم بشتغلوا لشعب الاميركي يكذبوا ويغشوا ويسرقوا ويقتلو شعوبهم ويعملوا لصالح اعداهم</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>TRAIN_2</td>\n",
              "      <td>ابشر طال عمرك</td>\n",
              "      <td>Saudi_Arabia</td>\n",
              "      <td>ابشر طال عمرك</td>\n",
              "      <td>ابشر طال عمر ك</td>\n",
              "      <td>ابشر طال عمرك</td>\n",
              "      <td>ابشر طال عمرك</td>\n",
              "      <td>ابشر طال عمرك</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>TRAIN_3</td>\n",
              "      <td>منطق 2017: أنا والغريب علي إبن عمي وأنا والغريب وإبن عمي علي أخويا. #قطع_العلاقات_مع_قطر #موريتانيا_مع_قطر</td>\n",
              "      <td>Mauritania</td>\n",
              "      <td>منطق  أنا والغريب علي إبن عمي وأنا والغريب وإبن عمي علي أخويا قطع العلاقات مع قطر موريتانيا مع قطر</td>\n",
              "      <td>منطق أنا و ال غريب علي إبن عمي و أنا و ال غريب و إبن عمي علي أخوي ا قطع ال علاق ات مع قطر موريتانيا مع قطر</td>\n",
              "      <td>منطق انا والغريب علي ابن عمي وانا والغريب وابن عمي علي اخويا قطع العلاقات مع قطر موريتانيا مع قطر</td>\n",
              "      <td>منطق والغريب ابن عمي وانا والغريب وابن عمي اخويا قطع العلاقات قطر موريتانيا قطر</td>\n",
              "      <td>منطق والغريب ابن عمي وانا وابن اخويا قطع العلاقات قطر موريتانيا</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>TRAIN_4</td>\n",
              "      <td>شهرين وتروح والباقي غير صيف ملينا</td>\n",
              "      <td>Algeria</td>\n",
              "      <td>شهرين وتروح والباقي غير صيف ملينا</td>\n",
              "      <td>شهر ين و تروح و ال باقي غير صيف ملي نا</td>\n",
              "      <td>شهرين وتروح والباقي غير صيف ملينا</td>\n",
              "      <td>شهرين وتروح والباقي صيف ملينا</td>\n",
              "      <td>شهرين وتروح والباقي صيف ملينا</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  #1_tweetid  ... #classes_id\n",
              "0  TRAIN_0    ...  0         \n",
              "1  TRAIN_1    ...  1         \n",
              "2  TRAIN_2    ...  2         \n",
              "3  TRAIN_3    ...  3         \n",
              "4  TRAIN_4    ...  4         \n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SuVE8s86C7D"
      },
      "source": [
        "classes_names = train_dataframe['#3_country_label'].unique().tolist()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gsjKH5cOKJN"
      },
      "source": [
        "train_x = train_dataframe['#2_tweet_clean_V0'].tolist()\n",
        "train_y = train_dataframe['#classes_id'].tolist()\n",
        "\n",
        "test_x = test_dataframe['#2_tweet_clean_V0'].tolist()\n",
        "test_y = test_dataframe['#classes_id'].tolist()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BOv-J32gY0g"
      },
      "source": [
        "train_tweets_lengths=[]\n",
        "for x in train_x:\n",
        "  if len(x) not in train_tweets_lengths:\n",
        "    train_tweets_lengths.append(len(x.split(' ')))\n",
        "\n",
        "test_tweets_lengths=[]\n",
        "for x in test_x:\n",
        "  if len(x) not in test_tweets_lengths:\n",
        "    test_tweets_lengths.append(len(x.split(' ')))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmrJDZVvfiXT",
        "outputId": "71f87da2-955f-4548-f0b5-6fffb67a2b5e"
      },
      "source": [
        "train_tweets_lengths = sorted(train_tweets_lengths)\n",
        "test_tweets_lengths = sorted(test_tweets_lengths)\n",
        "train_median_length = np.median(train_tweets_lengths)\n",
        "test_median_length = np.median(test_tweets_lengths)\n",
        "\n",
        "\n",
        "\n",
        "print(\"min train length: \", train_tweets_lengths[0], \",  min test length: \", test_tweets_lengths[0])\n",
        "print(\"max train length: \", train_tweets_lengths[-1], \",  max test length: \", test_tweets_lengths[-1])\n",
        "print(\"train median length: \", train_median_length, \",  test median length: \",test_median_length)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "min train length:  3 ,  min test length:  3\n",
            "max train length:  63 ,  max test length:  62\n",
            "train median length:  17.0 ,  test median length:  15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvNSEu8HOStw"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"bashar-talafha/multi-dialect-bert-base-arabic\")\n",
        "# tokenize and encode sequences in the training set\n",
        "tokens_train = tokenizer.batch_encode_plus(\n",
        "    train_x,\n",
        "    max_length = int(65),\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")\n",
        "\n",
        "# tokenize and encode sequences in the validation set\n",
        "tokens_val = tokenizer.batch_encode_plus(\n",
        "    test_x,\n",
        "    max_length = int(65),\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNyznyQHOWGg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5199ba64-11c1-4166-80ec-f7776a17f10f"
      },
      "source": [
        "# for train set\n",
        "train_seq = torch.tensor(tokens_train['input_ids'])\n",
        "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
        "train_y = torch.tensor(train_y)\n",
        "\n",
        "# for validation set\n",
        "val_seq = torch.tensor(tokens_val['input_ids'])\n",
        "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
        "val_y = torch.tensor(test_y)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8f4fVJG36RET"
      },
      "source": [
        "# train_dataloader"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ck_IudiPykX"
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "#define a batch size\n",
        "batch_size = 128\n",
        "# wrap tensors\n",
        "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
        "# sampler for sampling the data during training\n",
        "train_sampler = RandomSampler(train_data)\n",
        "# dataLoader for train set\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "# wrap tensors\n",
        "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
        "# sampler for sampling the data during training\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "# dataLoader for validation set\n",
        "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3R2HDinQDFq"
      },
      "source": [
        "bert = AutoModel.from_pretrained(\"bashar-talafha/multi-dialect-bert-base-arabic\")\n",
        "# freeze all the parameters\n",
        "for param in bert.parameters():\n",
        "    param.requires_grad = False"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6djfrF1XF3X8"
      },
      "source": [
        "# batch =  next(iter(train_dataloader)) \n",
        "#     # # progress update after every 50 batches.\n",
        "#     # # push the batch to gpu\n",
        "#     # # batch = [r.to(device) for r in batch]\n",
        "# sent_id, mask, labels = batch\n",
        "# preds = bert(sent_id, mask)\n",
        "\n",
        "#     # print(len(preds))\n",
        "\n",
        "#     # break"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jSADIcVH_Ex"
      },
      "source": [
        "# len(preds)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PN672pwDH-_E"
      },
      "source": [
        "# print(preds[0].shape, preds[1].shape)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4o3RFzN_HxgN"
      },
      "source": [
        ""
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKPtQiNIQKkL"
      },
      "source": [
        "class BERT_Arch(nn.Module):\n",
        "    def __init__(self, bert):\n",
        "        super(BERT_Arch, self).__init__()\n",
        "        self.bert = bert \n",
        "        # dropout layer\n",
        "        self.dropout = nn.Dropout(0.4)\n",
        "        # relu activation function\n",
        "        self.relu =  nn.ReLU()\n",
        "        # dense layer 1\n",
        "        self.fc1 = nn.Linear(768,512)\n",
        "        # dense layer 2 (Output layer)\n",
        "        self.fc2 = nn.Linear(512,21)\n",
        "        #softmax activation function\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    #define the forward pass\n",
        "    def forward(self, sent_id, mask):\n",
        "        #pass the inputs to the model  \n",
        "        _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n",
        "        x = self.fc1(cls_hs)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        # output layer\n",
        "        x = self.fc2(x)\n",
        "        # apply softmax activation\n",
        "        x = self.softmax(x)\n",
        "        return x"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrOLxDyOQWZq"
      },
      "source": [
        "device = 'cuda'\n",
        "model = BERT_Arch(bert)\n",
        "model = model.to(device)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mkm3nZf3QbW2",
        "outputId": "020cfbcf-67bb-4d6e-8082-c30eb514139d"
      },
      "source": [
        "# define the optimizer\n",
        "optimizer = AdamW(model.parameters(), lr = 1e-3)\n",
        "#compute the class weights\n",
        "y = train_dataframe['#3_country_label']\n",
        "class_weights = compute_class_weight('balanced', np.unique(y), y)\n",
        "\n",
        "print(class_weights)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.5533307  4.64540421 4.6671117  0.23351927 0.36638368 2.32811633\n",
            " 2.32811633 1.55087252 0.77724662 4.6671117  1.16814258 0.66628546\n",
            " 2.33902085 4.6671117  0.46692936 5.80675526 4.6671117  0.77603878\n",
            " 1.16270303 1.5557039  2.33355585]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLUyFx23Qzj4"
      },
      "source": [
        "# function for evaluating the model\n",
        "def evaluate():\n",
        "  print(\"\\nEvaluating...\")\n",
        "  # deactivate dropout layers\n",
        "  model.eval()\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  # empty list to save the model predictions\n",
        "  total_preds = []\n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(val_dataloader):\n",
        "    # Progress update every 50 batches.\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "      # Calculate elapsed time in minutes.\n",
        "      # elapsed = format_time(time.time() - t0)\n",
        "      # Report progress.\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
        "    # push the batch to gpu\n",
        "    batch = [t.to(device) for t in batch]\n",
        "    sent_id, mask, labels = batch\n",
        "    # deactivate autograd\n",
        "    with torch.no_grad():\n",
        "      # model predictions\n",
        "      preds = model(sent_id, mask)\n",
        "      # all_predictions.append(preds)\n",
        "      # compute the validation loss between actual and predicted values\n",
        "      loss = cross_entropy(preds,labels)\n",
        "      total_loss = total_loss + loss.item()\n",
        "      preds = preds.detach().cpu().numpy()\n",
        "      total_preds.append(preds)\n",
        "  # compute the validation loss of the epoch\n",
        "  avg_loss = total_loss / len(val_dataloader) \n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "  return avg_loss, total_preds\n",
        "\n",
        "\n",
        "\n",
        "# function to train the model\n",
        "def train():\n",
        "  global model\n",
        "  model.train()\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  # empty list to save model predictions\n",
        "  total_preds=[]\n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(train_dataloader):\n",
        "    # progress update after every 50 batches.\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
        "    # push the batch to gpu\n",
        "    batch = [r.to(device) for r in batch]\n",
        "    sent_id, mask, labels = batch\n",
        "    # clear previously calculated gradients \n",
        "    model.zero_grad()        \n",
        "    # get model predictions for the current batch\n",
        "    preds = model(sent_id, mask)\n",
        "    # compute the loss between actual and predicted values\n",
        "    loss = cross_entropy(preds, labels)\n",
        "    # add on to the total loss\n",
        "    total_loss = total_loss + loss.item()\n",
        "    # backward pass to calculate the gradients\n",
        "    loss.backward()\n",
        "    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    # update parameters\n",
        "    optimizer.step()\n",
        "    # model predictions are stored on GPU. So, push it to CPU\n",
        "    preds=preds.detach().cpu().numpy()\n",
        "    # append the model predictions\n",
        "    total_preds.append(preds)\n",
        "  # compute the training loss of the epoch\n",
        "  avg_loss = total_loss / len(train_dataloader)\n",
        "  \n",
        "  # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  #returns the loss and predictions\n",
        "  return avg_loss, total_preds"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0XWxHEi6Mx6"
      },
      "source": [
        ""
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJ3RL2IYRKb8",
        "outputId": "48bcd021-e69f-4240-b025-71d475696f0c"
      },
      "source": [
        "# converting list of class weights to a tensor\n",
        "# weights= torch.tensor(class_weights,dtype=torch.float)\n",
        "# push to GPU\n",
        "# weights = weights.to(device)\n",
        "# define the loss function\n",
        "# cross_entropy  = nn.NLLLoss(weight=weights) \n",
        "cross_entropy  = nn.NLLLoss() \n",
        "\n",
        "# number of training epochs\n",
        "epochs = 3\n",
        "# set initial loss to infinite\n",
        "best_valid_loss = float('inf')\n",
        "# empty lists to store training and validation loss of each epoch\n",
        "train_losses=[]\n",
        "valid_losses=[]\n",
        "#for each epoch\n",
        "for epoch in range(epochs):\n",
        "  print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
        "  #train model\n",
        "  train_loss, _ = train()\n",
        "  #evaluate model\n",
        "  valid_loss, all_prediction = evaluate()\n",
        "  val_prediction = np.argmax(all_prediction, axis=1)\n",
        "  print(classification_report(val_y, val_prediction, target_names=classes_names))\n",
        "  macro_f1 = f1_score(val_y, val_prediction, average='macro')\n",
        "  print(\"macro_f1:: \", macro_f1)\n",
        "  #save the best model\n",
        "  if macro_f1 > best_valid_loss:\n",
        "      best_valid_loss = macro_f1\n",
        "      torch.save(model.state_dict(), 'saved_weights_'+str(epoch)+'_f1_'+str(macro_f1)+'_.pt')\n",
        "  # append training and validation loss\n",
        "  train_losses.append(train_loss)\n",
        "  valid_losses.append(valid_loss)\n",
        "  print(f'\\nTraining Loss: {train_loss:.3f}')\n",
        "  print(f'Validation Loss: {valid_loss:.3f}')"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch 1 / 3\n",
            "  Batch    50  of    164.\n",
            "  Batch   100  of    164.\n",
            "  Batch   150  of    164.\n",
            "\n",
            "Evaluating...\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "               Egypt       0.59      0.89      0.71      1041\n",
            "                Iraq       0.35      0.46      0.40       663\n",
            "        Saudi_Arabia       0.32      0.49      0.39       519\n",
            "          Mauritania       0.25      0.04      0.07        53\n",
            "             Algeria       0.37      0.43      0.40       430\n",
            "               Syria       0.20      0.13      0.16       278\n",
            "                Oman       0.19      0.20      0.20       355\n",
            "             Tunisia       0.22      0.12      0.16       172\n",
            "             Lebanon       0.18      0.08      0.11       157\n",
            "             Morocco       0.22      0.05      0.08       207\n",
            "            Djibouti       0.00      0.00      0.00        27\n",
            "United_Arab_Emirates       0.29      0.24      0.26       157\n",
            "              Kuwait       0.10      0.04      0.06       105\n",
            "               Libya       0.33      0.16      0.21       314\n",
            "             Bahrain       0.00      0.00      0.00        52\n",
            "               Qatar       0.25      0.04      0.07        52\n",
            "               Yemen       0.50      0.11      0.19       105\n",
            "           Palestine       0.21      0.05      0.08       104\n",
            "              Jordan       0.00      0.00      0.00       104\n",
            "             Somalia       0.00      0.00      0.00        49\n",
            "               Sudan       0.30      0.34      0.32        53\n",
            "\n",
            "            accuracy                           0.39      4997\n",
            "           macro avg       0.23      0.18      0.18      4997\n",
            "        weighted avg       0.34      0.39      0.34      4997\n",
            "\n",
            "macro_f1::  0.1830152987915613\n",
            "\n",
            "Training Loss: 1.654\n",
            "Validation Loss: 2.346\n",
            "\n",
            " Epoch 2 / 3\n",
            "  Batch    50  of    164.\n",
            "  Batch   100  of    164.\n",
            "  Batch   150  of    164.\n",
            "\n",
            "Evaluating...\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "               Egypt       0.56      0.91      0.69      1041\n",
            "                Iraq       0.40      0.42      0.41       663\n",
            "        Saudi_Arabia       0.31      0.57      0.40       519\n",
            "          Mauritania       0.33      0.06      0.10        53\n",
            "             Algeria       0.41      0.43      0.42       430\n",
            "               Syria       0.22      0.15      0.18       278\n",
            "                Oman       0.20      0.18      0.19       355\n",
            "             Tunisia       0.28      0.10      0.15       172\n",
            "             Lebanon       0.20      0.09      0.12       157\n",
            "             Morocco       0.25      0.04      0.07       207\n",
            "            Djibouti       0.00      0.00      0.00        27\n",
            "United_Arab_Emirates       0.31      0.24      0.27       157\n",
            "              Kuwait       0.03      0.01      0.01       105\n",
            "               Libya       0.30      0.20      0.24       314\n",
            "             Bahrain       0.00      0.00      0.00        52\n",
            "               Qatar       0.25      0.02      0.04        52\n",
            "               Yemen       0.54      0.12      0.20       105\n",
            "           Palestine       0.21      0.08      0.11       104\n",
            "              Jordan       0.14      0.01      0.02       104\n",
            "             Somalia       0.00      0.00      0.00        49\n",
            "               Sudan       0.33      0.36      0.35        53\n",
            "\n",
            "            accuracy                           0.40      4997\n",
            "           macro avg       0.25      0.19      0.19      4997\n",
            "        weighted avg       0.35      0.40      0.35      4997\n",
            "\n",
            "macro_f1::  0.18974944118408227\n",
            "\n",
            "Training Loss: 1.648\n",
            "Validation Loss: 2.362\n",
            "\n",
            " Epoch 3 / 3\n",
            "  Batch    50  of    164.\n",
            "  Batch   100  of    164.\n",
            "  Batch   150  of    164.\n",
            "\n",
            "Evaluating...\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "               Egypt       0.56      0.91      0.69      1041\n",
            "                Iraq       0.38      0.44      0.41       663\n",
            "        Saudi_Arabia       0.31      0.56      0.40       519\n",
            "          Mauritania       0.60      0.06      0.10        53\n",
            "             Algeria       0.39      0.45      0.41       430\n",
            "               Syria       0.21      0.15      0.18       278\n",
            "                Oman       0.22      0.14      0.17       355\n",
            "             Tunisia       0.23      0.05      0.09       172\n",
            "             Lebanon       0.24      0.09      0.13       157\n",
            "             Morocco       0.24      0.04      0.07       207\n",
            "            Djibouti       0.00      0.00      0.00        27\n",
            "United_Arab_Emirates       0.28      0.29      0.29       157\n",
            "              Kuwait       0.04      0.01      0.02       105\n",
            "               Libya       0.28      0.17      0.21       314\n",
            "             Bahrain       0.00      0.00      0.00        52\n",
            "               Qatar       0.00      0.00      0.00        52\n",
            "               Yemen       0.52      0.12      0.20       105\n",
            "           Palestine       0.21      0.07      0.10       104\n",
            "              Jordan       0.27      0.03      0.05       104\n",
            "             Somalia       0.00      0.00      0.00        49\n",
            "               Sudan       0.38      0.34      0.36        53\n",
            "\n",
            "            accuracy                           0.40      4997\n",
            "           macro avg       0.25      0.19      0.18      4997\n",
            "        weighted avg       0.34      0.40      0.34      4997\n",
            "\n",
            "macro_f1::  0.18432065204205555\n",
            "\n",
            "Training Loss: 1.650\n",
            "Validation Loss: 2.328\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrggXiTwRVwJ"
      },
      "source": [
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtwEjwioxiYj"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsEX62o_xi_0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}