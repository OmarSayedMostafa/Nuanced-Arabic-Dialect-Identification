{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wO0QGfAxOjo0",
    "outputId": "a6311577-cceb-413e-d295-62e8fd02f5b6"
   },
   "outputs": [],
   "source": [
    "# !pip install plot-keras-history transformers babel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "70xT5-M-LQPX",
    "outputId": "b692f1cc-1d10-4a2a-9008-dcf7bbc1171e"
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/OmarSayedMostafa/Nuanced-Arabic-Dialect-Identification.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xBxkcGx0LT-P",
    "outputId": "b92fe830-8a10-4479-fc18-3fdb3834ff04"
   },
   "outputs": [],
   "source": [
    "# cd Nuanced-Arabic-Dialect-Identification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "jOYMtAeTGfXo"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "qC_hYT3vIqVP"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from utilities import clean_arabic_tweet\n",
    "# optimizer from hugging face transformers\n",
    "from transformers import AdamW\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "I7qMojB0Ka1R"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "train_data_path = './clean_/DA_train_labeled.tsv'\n",
    "test_data_path = './data/DA_dev_labeled.tsv'\n",
    "\n",
    "train_dataframe = pd.read_csv(train_data_path, sep='\\t')\n",
    "test_dataframe = pd.read_csv(test_data_path, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "vz9ZTToGKze1"
   },
   "outputs": [],
   "source": [
    "del train_dataframe['#4_province_label']\n",
    "del test_dataframe['#4_province_label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "ADTMc8niLsfB"
   },
   "outputs": [],
   "source": [
    "tarin_tweets_cleaned = (train_dataframe['#2_tweet'].apply(clean_arabic_tweet))\n",
    "test_tweets_cleaned = (test_dataframe['#2_tweet'].apply(clean_arabic_tweet))\n",
    "\n",
    "\n",
    "train_dataframe['#2_tweet_clean']= tarin_tweets_cleaned\n",
    "test_dataframe['#2_tweet_clean']= test_tweets_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "NVURBYicLxAy",
    "outputId": "ad2ea460-76ad-4905-ea24-246030ed5033"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#1_tweetid</th>\n",
       "      <th>#2_tweet</th>\n",
       "      <th>#3_country_label</th>\n",
       "      <th>#2_tweet_clean</th>\n",
       "      <th>classes_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_0</td>\n",
       "      <td>حاجة حلوة اكيد</td>\n",
       "      <td>Egypt</td>\n",
       "      <td>حاجة حلوة اكيد</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_1</td>\n",
       "      <td>عم بشتغلوا للشعب الاميركي اما نحن يكذبوا ويغشوا ويسرقوا ويقتلو شعوبهم ويعملوا لصالح اعدائهم</td>\n",
       "      <td>Iraq</td>\n",
       "      <td>عم بشتغلوا لشعب الاميركي اما نحن يكذبوا ويغشوا ويسرقوا ويقتلو شعوبهم ويعملوا لصالح اعدائهم</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_2</td>\n",
       "      <td>ابشر طال عمرك</td>\n",
       "      <td>Saudi_Arabia</td>\n",
       "      <td>ابشر طال عمرك</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_3</td>\n",
       "      <td>منطق 2017: أنا والغريب علي إبن عمي وأنا والغريب وإبن عمي علي أخويا. #قطع_العلاقات_مع_قطر #موريتانيا_مع_قطر</td>\n",
       "      <td>Mauritania</td>\n",
       "      <td>منطق انا والغريب علي ابن عمي وانا والغريب وابن عمي علي اخويا قطع العلاقات مع قطر موريتانيا مع قطر</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_4</td>\n",
       "      <td>شهرين وتروح والباقي غير صيف ملينا</td>\n",
       "      <td>Algeria</td>\n",
       "      <td>شهرين وتروح والباقي غير صيف ملينا</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TRAIN_5</td>\n",
       "      <td>يابنتى والله ما حد متغاظ ولا مفروس منك ولا بيحسدك انتى عره اساسا.</td>\n",
       "      <td>Syria</td>\n",
       "      <td>يابنتى واله ما حد متغاظ ولا مفروس منك ولا بيحسدك انتى عره اساسا</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TRAIN_6</td>\n",
       "      <td>نفس الوقت بأكد على صاحبتي ان اي هدف هتحطه وتخططله هيبوظ والأفضل التشاؤم واننا نتوقع الأسوء دايما والفشل عشان منعشمش نفسنا ع الفاضي</td>\n",
       "      <td>Egypt</td>\n",
       "      <td>نفس الوقت باكد على صاحبتي ان اي هدف هتحطه وتخطله هيبوظ والافضل التشاؤم وانا نتوقع الاسوء دايما والفشل عشان منعشمش نفسنا ع الفاضي</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TRAIN_7</td>\n",
       "      <td>م تبطلي خرا بقا علشان مطلعهوش عليكي احترمي نفسك URL  …</td>\n",
       "      <td>Egypt</td>\n",
       "      <td>م تبطلي خرا بقا علشان مطلعهوش عليكي احترمي نفسك</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>TRAIN_8</td>\n",
       "      <td>ما يله دخل !</td>\n",
       "      <td>Oman</td>\n",
       "      <td>ما يله دخل</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>TRAIN_9</td>\n",
       "      <td>هو حلو بس يتخربط ع طلاب المدراس ليك مايغيرونه عدنا</td>\n",
       "      <td>Iraq</td>\n",
       "      <td>هو حلو بس يتخربط ع طلاب المدراس ليك مايغيرونه عدنا</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  #1_tweetid  \\\n",
       "0  TRAIN_0     \n",
       "1  TRAIN_1     \n",
       "2  TRAIN_2     \n",
       "3  TRAIN_3     \n",
       "4  TRAIN_4     \n",
       "5  TRAIN_5     \n",
       "6  TRAIN_6     \n",
       "7  TRAIN_7     \n",
       "8  TRAIN_8     \n",
       "9  TRAIN_9     \n",
       "\n",
       "                                                                                                                             #2_tweet  \\\n",
       "0  حاجة حلوة اكيد                                                                                                                       \n",
       "1  عم بشتغلوا للشعب الاميركي اما نحن يكذبوا ويغشوا ويسرقوا ويقتلو شعوبهم ويعملوا لصالح اعدائهم                                          \n",
       "2  ابشر طال عمرك                                                                                                                        \n",
       "3  منطق 2017: أنا والغريب علي إبن عمي وأنا والغريب وإبن عمي علي أخويا. #قطع_العلاقات_مع_قطر #موريتانيا_مع_قطر                           \n",
       "4  شهرين وتروح والباقي غير صيف ملينا                                                                                                    \n",
       "5  يابنتى والله ما حد متغاظ ولا مفروس منك ولا بيحسدك انتى عره اساسا.                                                                    \n",
       "6  نفس الوقت بأكد على صاحبتي ان اي هدف هتحطه وتخططله هيبوظ والأفضل التشاؤم واننا نتوقع الأسوء دايما والفشل عشان منعشمش نفسنا ع الفاضي   \n",
       "7  م تبطلي خرا بقا علشان مطلعهوش عليكي احترمي نفسك URL  …                                                                               \n",
       "8  ما يله دخل !                                                                                                                         \n",
       "9  هو حلو بس يتخربط ع طلاب المدراس ليك مايغيرونه عدنا                                                                                   \n",
       "\n",
       "  #3_country_label  \\\n",
       "0  Egypt             \n",
       "1  Iraq              \n",
       "2  Saudi_Arabia      \n",
       "3  Mauritania        \n",
       "4  Algeria           \n",
       "5  Syria             \n",
       "6  Egypt             \n",
       "7  Egypt             \n",
       "8  Oman              \n",
       "9  Iraq              \n",
       "\n",
       "                                                                                                                     #2_tweet_clean  \\\n",
       "0  حاجة حلوة اكيد                                                                                                                     \n",
       "1  عم بشتغلوا لشعب الاميركي اما نحن يكذبوا ويغشوا ويسرقوا ويقتلو شعوبهم ويعملوا لصالح اعدائهم                                         \n",
       "2  ابشر طال عمرك                                                                                                                      \n",
       "3  منطق انا والغريب علي ابن عمي وانا والغريب وابن عمي علي اخويا قطع العلاقات مع قطر موريتانيا مع قطر                                  \n",
       "4  شهرين وتروح والباقي غير صيف ملينا                                                                                                  \n",
       "5  يابنتى واله ما حد متغاظ ولا مفروس منك ولا بيحسدك انتى عره اساسا                                                                    \n",
       "6  نفس الوقت باكد على صاحبتي ان اي هدف هتحطه وتخطله هيبوظ والافضل التشاؤم وانا نتوقع الاسوء دايما والفشل عشان منعشمش نفسنا ع الفاضي   \n",
       "7  م تبطلي خرا بقا علشان مطلعهوش عليكي احترمي نفسك                                                                                    \n",
       "8  ما يله دخل                                                                                                                         \n",
       "9  هو حلو بس يتخربط ع طلاب المدراس ليك مايغيرونه عدنا                                                                                 \n",
       "\n",
       "   classes_id  \n",
       "0  0           \n",
       "1  1           \n",
       "2  2           \n",
       "3  3           \n",
       "4  4           \n",
       "5  5           \n",
       "6  0           \n",
       "7  0           \n",
       "8  6           \n",
       "9  1           "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataframe.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert class name to class id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "6H4HZS-rMRqz"
   },
   "outputs": [],
   "source": [
    "classes_names = train_dataframe['#3_country_label'].unique().tolist()\n",
    "classes_map={}\n",
    "for i,class_name in enumerate(classes_names):\n",
    "    classes_map[class_name]=i\n",
    "\n",
    "\n",
    "def find_class_id_from_name(class_name):\n",
    "    return classes_map[class_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "2gsjKH5cOKJN"
   },
   "outputs": [],
   "source": [
    "train_dataframe['classes_id']=train_dataframe['#3_country_label'].apply(find_class_id_from_name)\n",
    "test_dataframe['classes_id']=test_dataframe['#3_country_label'].apply(find_class_id_from_name)\n",
    "\n",
    "\n",
    "train_x = train_dataframe['#2_tweet_clean'].tolist()\n",
    "train_y = train_dataframe['classes_id'].tolist()\n",
    "\n",
    "test_x = test_dataframe['#2_tweet_clean'].tolist()\n",
    "test_y = test_dataframe['classes_id'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## toknize the data for bert input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "wvNSEu8HOStw"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bashar-talafha/multi-dialect-bert-base-arabic\")\n",
    "# tokenize and encode sequences in the training set\n",
    "tokens_train = tokenizer.batch_encode_plus(\n",
    "    train_x,\n",
    "    max_length = 50,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the validation set\n",
    "tokens_val = tokenizer.batch_encode_plus(\n",
    "    test_x,\n",
    "    max_length = 50,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "pNyznyQHOWGg"
   },
   "outputs": [],
   "source": [
    "# for train set\n",
    "train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "train_y = torch.tensor(train_y)\n",
    "\n",
    "# for validation set\n",
    "val_seq = torch.tensor(tokens_val['input_ids'])\n",
    "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
    "val_y = torch.tensor(test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare data generator for pytorch training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "-Ck_IudiPykX"
   },
   "outputs": [],
   "source": [
    "#define a batch size\n",
    "batch_size = 32\n",
    "# wrap tensors\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "# sampler for sampling the data during training\n",
    "train_sampler = RandomSampler(train_data)\n",
    "# dataLoader for train set\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "# wrap tensors\n",
    "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
    "# sampler for sampling the data during training\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "# dataLoader for validation set\n",
    "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load arabert pretrained model and freeze its layer to avoid retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "C3R2HDinQDFq"
   },
   "outputs": [],
   "source": [
    "bert = AutoModel.from_pretrained(\"bashar-talafha/multi-dialect-bert-base-arabic\")\n",
    "# freeze all the parameters\n",
    "for param in bert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create the top layers for fine tuning arabert for classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "OKPtQiNIQKkL"
   },
   "outputs": [],
   "source": [
    "class BERT_Arch(nn.Module):\n",
    "    def __init__(self, bert):\n",
    "        super(BERT_Arch, self).__init__()\n",
    "        self.bert = bert \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        # relu activation function\n",
    "        self.relu =  nn.ReLU()\n",
    "        # dense layer 1\n",
    "        self.fc1 = nn.Linear(768,512)\n",
    "        # dense layer 2 (Output layer)\n",
    "        self.fc2 = nn.Linear(512,21)\n",
    "        #softmax activation function\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    #define the forward pass\n",
    "    def forward(self, sent_id, mask):\n",
    "        #pass the inputs to the model  \n",
    "        _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n",
    "        x = self.fc1(cls_hs)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        # output layer\n",
    "        x = self.fc2(x)\n",
    "        # apply softmax activation\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "jrOLxDyOQWZq"
   },
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "model = BERT_Arch(bert)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mkm3nZf3QbW2",
    "outputId": "97a18649-cd7a-432a-e242-f8416c7f354d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5527916  4.65116279 4.65116279 0.2334812  0.36643459 2.33100233\n",
      " 2.33100233 1.55279503 0.77760498 4.65116279 1.16550117 0.66622252\n",
      " 2.3364486  4.65116279 0.46728972 5.81395349 4.65116279 0.77700078\n",
      " 1.16414435 1.5576324  2.33100233]\n"
     ]
    }
   ],
   "source": [
    "# define the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr = 1e-3)\n",
    "#compute the class weights\n",
    "y = train_dataframe['#3_country_label']\n",
    "class_weights = compute_class_weight('balanced', np.unique(y), y)\n",
    "\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for evaluating the model\n",
    "def evaluate():\n",
    "    print(\"\\nEvaluating...\")\n",
    "    # deactivate dropout layers\n",
    "    model.eval()\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "    # empty list to save the model predictions\n",
    "    total_preds = []\n",
    "    # iterate over batches\n",
    "    for step,batch in enumerate(val_dataloader):\n",
    "        # Progress update every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "        # Calculate elapsed time in minutes.\n",
    "        # elapsed = format_time(time.time() - t0)\n",
    "        # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
    "        # push the batch to gpu\n",
    "        batch = [t.to(device) for t in batch]\n",
    "        sent_id, mask, labels = batch\n",
    "        # deactivate autograd\n",
    "        with torch.no_grad():\n",
    "            # model predictions\n",
    "            preds = model(sent_id, mask)\n",
    "            # all_predictions.append(preds)\n",
    "            # compute the validation loss between actual and predicted values\n",
    "            loss = cross_entropy(preds,labels)\n",
    "            total_loss = total_loss + loss.item()\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "        total_preds.append(preds)\n",
    "    # compute the validation loss of the epoch\n",
    "    avg_loss = total_loss / len(val_dataloader) \n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "wLUyFx23Qzj4"
   },
   "outputs": [],
   "source": [
    "# function to train the model\n",
    "def train():\n",
    "    global model\n",
    "    model.train()\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "    # empty list to save model predictions\n",
    "    total_preds=[]\n",
    "    # iterate over batches\n",
    "    for step,batch in enumerate(train_dataloader):\n",
    "        # progress update after every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
    "        # push the batch to gpu\n",
    "        batch = [r.to(device) for r in batch]\n",
    "        sent_id, mask, labels = batch\n",
    "        # clear previously calculated gradients \n",
    "        model.zero_grad()        \n",
    "        # get model predictions for the current batch\n",
    "        preds = model(sent_id, mask)\n",
    "        # compute the loss between actual and predicted values\n",
    "        loss = cross_entropy(preds, labels)\n",
    "        # add on to the total loss\n",
    "        total_loss = total_loss + loss.item()\n",
    "        # backward pass to calculate the gradients\n",
    "        loss.backward()\n",
    "        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        # model predictions are stored on GPU. So, push it to CPU\n",
    "        preds=preds.detach().cpu().numpy()\n",
    "        # append the model predictions\n",
    "        total_preds.append(preds)\n",
    "    # compute the training loss of the epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "  \n",
    "    # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    #returns the loss and predictions\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oJ3RL2IYRKb8",
    "outputId": "f3592159-1736-404d-d898-a0069d5edba9",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 150\n",
      "  Batch    50  of    657.\n",
      "  Batch   100  of    657.\n",
      "  Batch   150  of    657.\n",
      "  Batch   200  of    657.\n",
      "  Batch   250  of    657.\n",
      "  Batch   300  of    657.\n",
      "  Batch   350  of    657.\n",
      "  Batch   400  of    657.\n",
      "  Batch   450  of    657.\n",
      "  Batch   500  of    657.\n",
      "  Batch   550  of    657.\n",
      "  Batch   600  of    657.\n",
      "  Batch   650  of    657.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    157.\n",
      "  Batch   100  of    157.\n",
      "  Batch   150  of    157.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omar/anaconda3/envs/NLP/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/omar/anaconda3/envs/NLP/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/omar/anaconda3/envs/NLP/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      precision    recall  f1-score   support\n",
      "\n",
      "               Egypt       0.76      0.60      0.67      1041\n",
      "                Iraq       0.25      0.66      0.37       664\n",
      "        Saudi_Arabia       0.26      0.68      0.38       520\n",
      "          Mauritania       0.00      0.00      0.00        53\n",
      "             Algeria       0.00      0.00      0.00       430\n",
      "               Syria       0.20      0.10      0.14       278\n",
      "                Oman       0.20      0.01      0.01       355\n",
      "             Tunisia       0.31      0.02      0.04       173\n",
      "             Lebanon       0.00      0.00      0.00       157\n",
      "             Morocco       0.14      0.20      0.17       207\n",
      "            Djibouti       0.00      0.00      0.00        27\n",
      "United_Arab_Emirates       0.00      0.00      0.00       157\n",
      "              Kuwait       0.00      0.00      0.00       105\n",
      "               Libya       0.15      0.29      0.20       314\n",
      "             Bahrain       0.00      0.00      0.00        52\n",
      "               Qatar       0.00      0.00      0.00        52\n",
      "               Yemen       0.60      0.03      0.05       105\n",
      "           Palestine       0.00      0.00      0.00       104\n",
      "              Jordan       0.00      0.00      0.00       104\n",
      "             Somalia       0.00      0.00      0.00        49\n",
      "               Sudan       0.31      0.17      0.22        53\n",
      "\n",
      "            accuracy                           0.32      5000\n",
      "           macro avg       0.15      0.13      0.11      5000\n",
      "        weighted avg       0.29      0.32      0.26      5000\n",
      "\n",
      "\n",
      "Training Loss: 2.092\n",
      "Validation Loss: 2.015\n",
      "\n",
      " Epoch 2 / 150\n",
      "  Batch    50  of    657.\n",
      "  Batch   100  of    657.\n",
      "  Batch   150  of    657.\n",
      "  Batch   200  of    657.\n",
      "  Batch   250  of    657.\n",
      "  Batch   300  of    657.\n",
      "  Batch   350  of    657.\n",
      "  Batch   400  of    657.\n",
      "  Batch   450  of    657.\n",
      "  Batch   500  of    657.\n",
      "  Batch   550  of    657.\n",
      "  Batch   600  of    657.\n",
      "  Batch   650  of    657.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    157.\n",
      "  Batch   100  of    157.\n",
      "  Batch   150  of    157.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omar/anaconda3/envs/NLP/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/omar/anaconda3/envs/NLP/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/omar/anaconda3/envs/NLP/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      precision    recall  f1-score   support\n",
      "\n",
      "               Egypt       0.69      0.76      0.72      1041\n",
      "                Iraq       0.29      0.63      0.40       664\n",
      "        Saudi_Arabia       0.26      0.70      0.38       520\n",
      "          Mauritania       0.00      0.00      0.00        53\n",
      "             Algeria       0.00      0.00      0.00       430\n",
      "               Syria       0.20      0.13      0.16       278\n",
      "                Oman       0.56      0.01      0.03       355\n",
      "             Tunisia       1.00      0.01      0.01       173\n",
      "             Lebanon       0.00      0.00      0.00       157\n",
      "             Morocco       0.15      0.17      0.16       207\n",
      "            Djibouti       0.00      0.00      0.00        27\n",
      "United_Arab_Emirates       0.00      0.00      0.00       157\n",
      "              Kuwait       0.00      0.00      0.00       105\n",
      "               Libya       0.19      0.36      0.25       314\n",
      "             Bahrain       0.00      0.00      0.00        52\n",
      "               Qatar       0.00      0.00      0.00        52\n",
      "               Yemen       0.50      0.02      0.04       105\n",
      "           Palestine       0.00      0.00      0.00       104\n",
      "              Jordan       0.00      0.00      0.00       104\n",
      "             Somalia       0.00      0.00      0.00        49\n",
      "               Sudan       0.31      0.19      0.24        53\n",
      "\n",
      "            accuracy                           0.35      5000\n",
      "           macro avg       0.20      0.14      0.11      5000\n",
      "        weighted avg       0.33      0.35      0.28      5000\n",
      "\n",
      "\n",
      "Training Loss: 2.020\n",
      "Validation Loss: 2.015\n",
      "\n",
      " Epoch 3 / 150\n",
      "  Batch    50  of    657.\n",
      "  Batch   100  of    657.\n",
      "  Batch   150  of    657.\n",
      "  Batch   200  of    657.\n",
      "  Batch   250  of    657.\n",
      "  Batch   300  of    657.\n",
      "  Batch   350  of    657.\n",
      "  Batch   400  of    657.\n",
      "  Batch   450  of    657.\n",
      "  Batch   500  of    657.\n",
      "  Batch   550  of    657.\n",
      "  Batch   600  of    657.\n",
      "  Batch   650  of    657.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    157.\n",
      "  Batch   100  of    157.\n",
      "  Batch   150  of    157.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omar/anaconda3/envs/NLP/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/omar/anaconda3/envs/NLP/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/omar/anaconda3/envs/NLP/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      precision    recall  f1-score   support\n",
      "\n",
      "               Egypt       0.73      0.72      0.72      1041\n",
      "                Iraq       0.26      0.67      0.38       664\n",
      "        Saudi_Arabia       0.24      0.65      0.35       520\n",
      "          Mauritania       0.00      0.00      0.00        53\n",
      "             Algeria       0.00      0.00      0.00       430\n",
      "               Syria       0.20      0.16      0.18       278\n",
      "                Oman       0.00      0.00      0.00       355\n",
      "             Tunisia       0.21      0.02      0.04       173\n",
      "             Lebanon       0.00      0.00      0.00       157\n",
      "             Morocco       0.21      0.17      0.19       207\n",
      "            Djibouti       0.00      0.00      0.00        27\n",
      "United_Arab_Emirates       0.00      0.00      0.00       157\n",
      "              Kuwait       0.00      0.00      0.00       105\n",
      "               Libya       0.20      0.27      0.23       314\n",
      "             Bahrain       0.00      0.00      0.00        52\n",
      "               Qatar       0.00      0.00      0.00        52\n",
      "               Yemen       0.44      0.13      0.20       105\n",
      "           Palestine       0.00      0.00      0.00       104\n",
      "              Jordan       0.00      0.00      0.00       104\n",
      "             Somalia       0.00      0.00      0.00        49\n",
      "               Sudan       0.48      0.23      0.31        53\n",
      "\n",
      "            accuracy                           0.34      5000\n",
      "           macro avg       0.14      0.14      0.12      5000\n",
      "        weighted avg       0.27      0.34      0.28      5000\n",
      "\n",
      "\n",
      "Training Loss: 1.998\n",
      "Validation Loss: 2.019\n",
      "\n",
      " Epoch 4 / 150\n",
      "  Batch    50  of    657.\n",
      "  Batch   100  of    657.\n",
      "  Batch   150  of    657.\n",
      "  Batch   200  of    657.\n",
      "  Batch   250  of    657.\n",
      "  Batch   300  of    657.\n",
      "  Batch   350  of    657.\n",
      "  Batch   400  of    657.\n",
      "  Batch   450  of    657.\n",
      "  Batch   500  of    657.\n",
      "  Batch   550  of    657.\n",
      "  Batch   600  of    657.\n",
      "  Batch   650  of    657.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    157.\n",
      "  Batch   100  of    157.\n",
      "  Batch   150  of    157.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omar/anaconda3/envs/NLP/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/omar/anaconda3/envs/NLP/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/omar/anaconda3/envs/NLP/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      precision    recall  f1-score   support\n",
      "\n",
      "               Egypt       0.74      0.69      0.71      1041\n",
      "                Iraq       0.31      0.58      0.41       664\n",
      "        Saudi_Arabia       0.25      0.75      0.37       520\n",
      "          Mauritania       0.00      0.00      0.00        53\n",
      "             Algeria       0.00      0.00      0.00       430\n",
      "               Syria       0.20      0.17      0.18       278\n",
      "                Oman       1.00      0.01      0.02       355\n",
      "             Tunisia       0.50      0.01      0.02       173\n",
      "             Lebanon       0.50      0.01      0.01       157\n",
      "             Morocco       0.14      0.20      0.16       207\n",
      "            Djibouti       0.00      0.00      0.00        27\n",
      "United_Arab_Emirates       0.00      0.00      0.00       157\n",
      "              Kuwait       0.00      0.00      0.00       105\n",
      "               Libya       0.20      0.38      0.26       314\n",
      "             Bahrain       0.00      0.00      0.00        52\n",
      "               Qatar       0.00      0.00      0.00        52\n",
      "               Yemen       0.45      0.17      0.25       105\n",
      "           Palestine       0.00      0.00      0.00       104\n",
      "              Jordan       0.00      0.00      0.00       104\n",
      "             Somalia       0.00      0.00      0.00        49\n",
      "               Sudan       0.29      0.09      0.14        53\n",
      "\n",
      "            accuracy                           0.35      5000\n",
      "           macro avg       0.22      0.15      0.12      5000\n",
      "        weighted avg       0.37      0.35      0.28      5000\n",
      "\n",
      "\n",
      "Training Loss: 1.980\n",
      "Validation Loss: 1.985\n",
      "\n",
      " Epoch 5 / 150\n",
      "  Batch    50  of    657.\n",
      "  Batch   100  of    657.\n",
      "  Batch   150  of    657.\n",
      "  Batch   200  of    657.\n",
      "  Batch   250  of    657.\n",
      "  Batch   300  of    657.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-12788a2cbebd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n Epoch {:} / {:}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m#train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;31m#evaluate model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_prediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-41-101d13f2dfaf>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# add on to the total loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;31m# backward pass to calculate the gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# converting list of class weights to a tensor\n",
    "weights= torch.tensor(class_weights,dtype=torch.float)\n",
    "# push to GPU\n",
    "weights = weights.to(device)\n",
    "# define the loss function\n",
    "cross_entropy  = nn.NLLLoss(weight=weights) \n",
    "# number of training epochs\n",
    "epochs = 150\n",
    "# set initial loss to infinite\n",
    "best_valid_loss = float('inf')\n",
    "# empty lists to store training and validation loss of each epoch\n",
    "train_losses=[]\n",
    "valid_losses=[]\n",
    "#for each epoch\n",
    "for epoch in range(epochs):\n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "    #train model\n",
    "    train_loss, _ = train()\n",
    "    #evaluate model\n",
    "    valid_loss, all_prediction = evaluate()\n",
    "    print(classification_report(val_y, np.argmax(all_prediction, axis=1), target_names=classes_names))\n",
    "    #save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
    "    # append training and validation loss\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
    "    print(f'Validation Loss: {valid_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BrggXiTwRVwJ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "AraBert.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
