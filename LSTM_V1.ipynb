{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mena\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: (20974, 8) 20974\n",
      "Test set: (4997, 8) 4997\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import gensim\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_recall_curve, precision_score, recall_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from utilities import remove_empty_tweets\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "\n",
    "#text is already cleaned.\n",
    "#assign cleaned data to these variables.\n",
    "train_data_path = 'cleaned_data/cleaned_train_data_for_subtask1.csv'\n",
    "test_data_path = 'cleaned_data/cleaned_test_data_for_subtask1.csv'\n",
    "#read files.\n",
    "train_data = pd.read_csv(train_data_path)\n",
    "test_data = pd.read_csv(test_data_path)\n",
    "\n",
    "print(\"Train set:\"% train_data.columns, train_data.shape, len(train_data)) \n",
    "print(\"Test set:\"% test_data.columns, test_data.shape, len(test_data)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove empty tweets.\n",
    "train_data = remove_empty_tweets(train_data, \"#2_tweet_clean_V1\")\n",
    "test = remove_empty_tweets(test_data, \"#2_tweet_clean_V1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare train and test data.\n",
    "X_train = train_data['#2_tweet_clean_V1'].tolist()\n",
    "y_train = train_data['#classes_id'].tolist()\n",
    "X_test = test_data['#2_tweet_clean_V1'].tolist()\n",
    "y_test = test_data['#classes_id'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 49126 unique tokens.\n",
      "Shape of data tensor: (20974, 500)\n",
      "Shape of data tensor: (4997, 500)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#tweets tokenize\n",
    "max_no = 80000 #max common words\n",
    "sql_len = 500 #max length\n",
    "dim = 64\n",
    "\n",
    "tokenizer = Tokenizer(lower=False, num_words=max_no)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "#padding\n",
    "X_tr = tokenizer.texts_to_sequences(X_train)\n",
    "X_tr = pad_sequences(X_tr, maxlen=sql_len)\n",
    "print('Shape of data tensor:', X_tr.shape)\n",
    "\n",
    "X_te = tokenizer.texts_to_sequences(X_test)\n",
    "X_te = pad_sequences(X_te, maxlen=sql_len)\n",
    "print('Shape of data tensor:', X_te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 128)          6400000   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 256)               263168    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 21)                2709      \n",
      "=================================================================\n",
      "Total params: 6,698,773\n",
      "Trainable params: 6,698,773\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\Mena\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 16779 samples, validate on 4195 samples\n",
      "Epoch 1/10\n",
      "16779/16779 [==============================] - 862s 51ms/step - loss: 2.7011 - accuracy: 0.1949 - val_loss: 2.6408 - val_accuracy: 0.1957\n",
      "Epoch 2/10\n",
      "16779/16779 [==============================] - 1121s 67ms/step - loss: 2.5082 - accuracy: 0.2463 - val_loss: 2.5038 - val_accuracy: 0.2570\n",
      "Epoch 3/10\n",
      "16779/16779 [==============================] - 2365s 141ms/step - loss: 2.1142 - accuracy: 0.3783 - val_loss: 2.4395 - val_accuracy: 0.2999\n",
      "Epoch 4/10\n",
      "16779/16779 [==============================] - 2267s 135ms/step - loss: 1.5767 - accuracy: 0.5211 - val_loss: 2.5748 - val_accuracy: 0.3094\n",
      "Epoch 5/10\n",
      "16779/16779 [==============================] - 1555s 93ms/step - loss: 1.2006 - accuracy: 0.6304 - val_loss: 2.9416 - val_accuracy: 0.3018\n",
      "Epoch 6/10\n",
      "16779/16779 [==============================] - 1052s 63ms/step - loss: 0.9029 - accuracy: 0.7238 - val_loss: 3.1892 - val_accuracy: 0.2887\n",
      "Epoch 7/10\n",
      "16779/16779 [==============================] - 2532s 151ms/step - loss: 0.6685 - accuracy: 0.7941 - val_loss: 3.6471 - val_accuracy: 0.2903\n",
      "Epoch 8/10\n",
      "16779/16779 [==============================] - 3010s 179ms/step - loss: 0.4924 - accuracy: 0.8486 - val_loss: 3.9715 - val_accuracy: 0.2834\n",
      "Epoch 9/10\n",
      "16779/16779 [==============================] - 2401s 143ms/step - loss: 0.3651 - accuracy: 0.8903 - val_loss: 4.3020 - val_accuracy: 0.2903\n",
      "Epoch 10/10\n",
      "16779/16779 [==============================] - 4279s 255ms/step - loss: 0.2707 - accuracy: 0.9245 - val_loss: 4.6415 - val_accuracy: 0.2806\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.optimizers import adam, SGD\n",
    "from keras.layers import Embedding, GlobalAveragePooling1D, SpatialDropout1D, LSTM, Dense, Dropout, Bidirectional\n",
    "import keras\n",
    "import tensorflow\n",
    "from keras.layers import Embedding, SpatialDropout1D, LSTM, Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "vocab_size = 50000\n",
    "embedding_dim = 128\n",
    "\n",
    "adam_opt = adam(lr=0.001, decay=1e-6)\n",
    "sdg_opt = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_shape=X_tr.shape[1:]))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Bidirectional(LSTM(embedding_dim)))\n",
    "model.add(Dense(embedding_dim, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(21,activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam_opt, metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 256\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "Y_tr = to_categorical(y_train)\n",
    "Y_te = to_categorical(y_test)\n",
    "\n",
    "history = model.fit(X_tr, Y_tr, epochs=epochs, batch_size=batch_size,validation_split=0.2, callbacks=[EarlyStopping(monitor='val_loss',patience=7, min_delta=0.0001)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13450067821803358\n"
     ]
    }
   ],
   "source": [
    "y_pred= model.predict(X_te)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "y_test = test_data['#classes_id']\n",
    "Y_te = pd.get_dummies(y_test).values\n",
    "\n",
    "Y_te = np.argmax(Y_te, axis=1)\n",
    "print(f1_score(Y_te, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      precision    recall  f1-score   support\n",
      "\n",
      "               Egypt       0.54      0.58      0.56      1041\n",
      "                Iraq       0.38      0.38      0.38       663\n",
      "        Saudi_Arabia       0.21      0.25      0.23       519\n",
      "          Mauritania       0.50      0.11      0.18        53\n",
      "             Algeria       0.35      0.31      0.33       430\n",
      "               Syria       0.10      0.11      0.10       278\n",
      "                Oman       0.14      0.14      0.14       355\n",
      "             Tunisia       0.20      0.12      0.15       172\n",
      "             Lebanon       0.07      0.04      0.05       157\n",
      "             Morocco       0.09      0.09      0.09       207\n",
      "            Djibouti       0.00      0.00      0.00        27\n",
      "United_Arab_Emirates       0.07      0.08      0.08       157\n",
      "              Kuwait       0.09      0.06      0.07       105\n",
      "               Libya       0.19      0.27      0.23       314\n",
      "             Bahrain       0.04      0.06      0.05        52\n",
      "               Qatar       0.02      0.02      0.02        52\n",
      "               Yemen       0.02      0.01      0.01       105\n",
      "           Palestine       0.04      0.06      0.04       104\n",
      "              Jordan       0.02      0.01      0.01       104\n",
      "             Somalia       0.06      0.04      0.05        49\n",
      "               Sudan       0.04      0.04      0.04        53\n",
      "\n",
      "         avg / total       0.27      0.27      0.27      4997\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classes = list(train_data['#3_country_label'].unique())\n",
    "print(classification_report(Y_te, y_pred,target_names=classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
