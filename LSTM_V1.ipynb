{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mena\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: (20974, 7) 20974\n",
      "Test set: (4997, 7) 4997\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import gensim\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_recall_curve, precision_score, recall_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from utilities import remove_empty_tweets\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "\n",
    "#text is already cleaned.\n",
    "#assign cleaned data to these variables.\n",
    "train_data_path = 'cleaned_data/cleaned_train_data_for_subtask1.csv'\n",
    "test_data_path = 'cleaned_data/cleaned_test_data_for_subtask1.csv'\n",
    "#read files.\n",
    "train_data = pd.read_csv(train_data_path)\n",
    "test_data = pd.read_csv(test_data_path)\n",
    "\n",
    "print(\"Train set:\"% train_data.columns, train_data.shape, len(train_data)) \n",
    "print(\"Test set:\"% test_data.columns, test_data.shape, len(test_data)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#1_tweetid</th>\n",
       "      <th>#2_tweet</th>\n",
       "      <th>#3_country_label</th>\n",
       "      <th>#2_tweet_clean_V1</th>\n",
       "      <th>#2_tweet_clean_V2</th>\n",
       "      <th>#2_tweet_clean_V3</th>\n",
       "      <th>#classes_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DEV_0</td>\n",
       "      <td>قولنا اون لاين لا يا علي اون لاين لا</td>\n",
       "      <td>Egypt</td>\n",
       "      <td>قولنا اون لاين لا يا علي اون لاين لا</td>\n",
       "      <td>قولنا اون لاين يا اون لاين</td>\n",
       "      <td>قولنا اون لاين يا</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DEV_1</td>\n",
       "      <td>ههههه بايخه ههههه URL  …</td>\n",
       "      <td>Oman</td>\n",
       "      <td>بايخه</td>\n",
       "      <td>بايخه</td>\n",
       "      <td>بايخه</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DEV_2</td>\n",
       "      <td>ربنا يخليك يا دوك ولك المثل :D</td>\n",
       "      <td>Lebanon</td>\n",
       "      <td>ربنا يخليك يا دوك ولك المثل</td>\n",
       "      <td>ربنا يخليك يا دوك ولك المثل</td>\n",
       "      <td>ربنا يخليك يا دوك ولك المثل</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DEV_3</td>\n",
       "      <td>#اوامر_ملكيه ياشباب اي واحد فيكم عنده شي يذكره...</td>\n",
       "      <td>Syria</td>\n",
       "      <td>اوامر ملكيه ياشباب اي واحد فيكم عنده شي يذكره ...</td>\n",
       "      <td>اوامر ملكيه ياشباب فيكم عنده شي يذكره بالعساف ...</td>\n",
       "      <td>اوامر ملكيه ياشباب فيكم عنده شي يذكره بالعساف ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DEV_4</td>\n",
       "      <td>شد عالخط حتى هيا اكويسه</td>\n",
       "      <td>Libya</td>\n",
       "      <td>شد عالخط حتى هيا اكويسه</td>\n",
       "      <td>شد عالخط اكويسه</td>\n",
       "      <td>شد عالخط اكويسه</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4992</th>\n",
       "      <td>DEV_4995</td>\n",
       "      <td>و في الشتاء مستحيل يقب</td>\n",
       "      <td>Libya</td>\n",
       "      <td>في الشتاء مستحيل يقب</td>\n",
       "      <td>الشتاء مستحيل يقب</td>\n",
       "      <td>الشتاء مستحيل يقب</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4993</th>\n",
       "      <td>DEV_4996</td>\n",
       "      <td>اكيد حبيبتي وانتِ في كيفك وكيف طيبة قلبك</td>\n",
       "      <td>Libya</td>\n",
       "      <td>اكيد حبيبتي وانت في كيفك وكيف طيبه قلبك</td>\n",
       "      <td>اكيد حبيبتي وانت كيفك وكيف طيبه قلبك</td>\n",
       "      <td>اكيد حبيبتي وانت كيفك وكيف طيبه قلبك</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4994</th>\n",
       "      <td>DEV_4997</td>\n",
       "      <td>يسعد صباح الناس رايقه</td>\n",
       "      <td>Jordan</td>\n",
       "      <td>يسعد صباح الناس رايقه</td>\n",
       "      <td>يسعد الناس رايقه</td>\n",
       "      <td>يسعد الناس رايقه</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>DEV_4998</td>\n",
       "      <td>فرحتهم فيهاا ياتقبرنييي  USER .twitter.com/hUS...</td>\n",
       "      <td>Morocco</td>\n",
       "      <td>فرحتهم فيها ياتقبرني</td>\n",
       "      <td>فرحتهم ياتقبرني</td>\n",
       "      <td>فرحتهم ياتقبرني</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>DEV_4999</td>\n",
       "      <td>والله عني انا لااا ومية لااذا استمر سييرا بيست...</td>\n",
       "      <td>Yemen</td>\n",
       "      <td>واله عني انا لا وميه لاذا استمر سيرا بيستمر مس...</td>\n",
       "      <td>واله عني وميه لاذا استمر سيرا بيستمر مستوى الا...</td>\n",
       "      <td>واله عني وميه لاذا استمر سيرا بيستمر مستوى الا...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4997 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     #1_tweetid                                           #2_tweet  \\\n",
       "0         DEV_0               قولنا اون لاين لا يا علي اون لاين لا   \n",
       "1         DEV_1                           ههههه بايخه ههههه URL  …   \n",
       "2         DEV_2                     ربنا يخليك يا دوك ولك المثل :D   \n",
       "3         DEV_3  #اوامر_ملكيه ياشباب اي واحد فيكم عنده شي يذكره...   \n",
       "4         DEV_4                            شد عالخط حتى هيا اكويسه   \n",
       "...         ...                                                ...   \n",
       "4992   DEV_4995                             و في الشتاء مستحيل يقب   \n",
       "4993   DEV_4996           اكيد حبيبتي وانتِ في كيفك وكيف طيبة قلبك   \n",
       "4994   DEV_4997                              يسعد صباح الناس رايقه   \n",
       "4995   DEV_4998  فرحتهم فيهاا ياتقبرنييي  USER .twitter.com/hUS...   \n",
       "4996   DEV_4999  والله عني انا لااا ومية لااذا استمر سييرا بيست...   \n",
       "\n",
       "     #3_country_label                                  #2_tweet_clean_V1  \\\n",
       "0               Egypt               قولنا اون لاين لا يا علي اون لاين لا   \n",
       "1                Oman                                              بايخه   \n",
       "2             Lebanon                        ربنا يخليك يا دوك ولك المثل   \n",
       "3               Syria  اوامر ملكيه ياشباب اي واحد فيكم عنده شي يذكره ...   \n",
       "4               Libya                            شد عالخط حتى هيا اكويسه   \n",
       "...               ...                                                ...   \n",
       "4992            Libya                               في الشتاء مستحيل يقب   \n",
       "4993            Libya            اكيد حبيبتي وانت في كيفك وكيف طيبه قلبك   \n",
       "4994           Jordan                              يسعد صباح الناس رايقه   \n",
       "4995          Morocco                               فرحتهم فيها ياتقبرني   \n",
       "4996            Yemen  واله عني انا لا وميه لاذا استمر سيرا بيستمر مس...   \n",
       "\n",
       "                                      #2_tweet_clean_V2  \\\n",
       "0                            قولنا اون لاين يا اون لاين   \n",
       "1                                                 بايخه   \n",
       "2                           ربنا يخليك يا دوك ولك المثل   \n",
       "3     اوامر ملكيه ياشباب فيكم عنده شي يذكره بالعساف ...   \n",
       "4                                       شد عالخط اكويسه   \n",
       "...                                                 ...   \n",
       "4992                                  الشتاء مستحيل يقب   \n",
       "4993               اكيد حبيبتي وانت كيفك وكيف طيبه قلبك   \n",
       "4994                                   يسعد الناس رايقه   \n",
       "4995                                    فرحتهم ياتقبرني   \n",
       "4996  واله عني وميه لاذا استمر سيرا بيستمر مستوى الا...   \n",
       "\n",
       "                                      #2_tweet_clean_V3  #classes_id  \n",
       "0                                     قولنا اون لاين يا            0  \n",
       "1                                                 بايخه            6  \n",
       "2                           ربنا يخليك يا دوك ولك المثل            8  \n",
       "3     اوامر ملكيه ياشباب فيكم عنده شي يذكره بالعساف ...            5  \n",
       "4                                       شد عالخط اكويسه           13  \n",
       "...                                                 ...          ...  \n",
       "4992                                  الشتاء مستحيل يقب           13  \n",
       "4993               اكيد حبيبتي وانت كيفك وكيف طيبه قلبك           13  \n",
       "4994                                   يسعد الناس رايقه           18  \n",
       "4995                                    فرحتهم ياتقبرني            9  \n",
       "4996  واله عني وميه لاذا استمر سيرا بيستمر مستوى الا...           16  \n",
       "\n",
       "[4997 rows x 7 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove empty tweets.\n",
    "remove_empty_tweets(train_data, \"#2_tweet_clean_V1\")\n",
    "remove_empty_tweets(test_data, \"#2_tweet_clean_V1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare train and test data.\n",
    "X_train = train_data['#2_tweet_clean_V1']\n",
    "y_train = train_data['#classes_id']\n",
    "X_test = test_data['#2_tweet_clean_V1']\n",
    "y_test = test_data['#classes_id']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 49126 unique tokens.\n",
      "Shape of data tensor: (20974, 250)\n",
      "Found 17491 unique tokens.\n",
      "Shape of data tensor: (4997, 250)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_no = 50000\n",
    "sql_len = 250\n",
    "dim = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_no, lower=False)\n",
    "tokenizer.fit_on_texts(X_train.values)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "X_tr = tokenizer.texts_to_sequences(X_train.values)\n",
    "X_tr = pad_sequences(X_tr, maxlen=sql_len)\n",
    "print('Shape of data tensor:', X_tr.shape)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_no, lower=False)\n",
    "tokenizer.fit_on_texts(X_test.values)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "X_te = tokenizer.texts_to_sequences(X_test.values)\n",
    "X_te = pad_sequences(X_te, maxlen=sql_len)\n",
    "print('Shape of data tensor:', X_te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of label tensor: (20974, 21)\n",
      "Shape of label tensor: (4997, 21)\n"
     ]
    }
   ],
   "source": [
    "Y_tr = pd.get_dummies(y_train).values\n",
    "print('Shape of label tensor:', Y_tr.shape)\n",
    "\n",
    "Y_te = pd.get_dummies(y_test).values\n",
    "print('Shape of label tensor:', Y_te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, None, 64)          3200000   \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 128)               66048     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 3,274,694\n",
      "Trainable params: 3,274,694\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "A target array with shape (20974, 21) was passed for an output of shape (None, 6) while using as loss `categorical_crossentropy`. This loss expects targets to have the same shape as the output.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-438405bdb1dd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    707\u001b[0m         \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    708\u001b[0m         \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 709\u001b[1;33m         shuffle=shuffle)\n\u001b[0m\u001b[0;32m    710\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    711\u001b[0m     \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2690\u001b[0m           \u001b[1;31m# Additional checks to avoid users mistakenly using improper loss fns.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2691\u001b[0m           training_utils.check_loss_and_target_compatibility(\n\u001b[1;32m-> 2692\u001b[1;33m               y, self._feed_loss_fns, feed_output_shapes)\n\u001b[0m\u001b[0;32m   2693\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2694\u001b[0m       \u001b[1;31m# If sample weight mode has not been set and weights are None for all the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mcheck_loss_and_target_compatibility\u001b[1;34m(targets, loss_fns, output_shapes)\u001b[0m\n\u001b[0;32m    547\u001b[0m           raise ValueError('A target array with shape ' + str(y.shape) +\n\u001b[0;32m    548\u001b[0m                            \u001b[1;34m' was passed for an output of shape '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 549\u001b[1;33m                            \u001b[1;34m' while using as loss `'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mloss_name\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'`. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    550\u001b[0m                            \u001b[1;34m'This loss expects targets to have the same shape '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m                            'as the output.')\n",
      "\u001b[1;31mValueError\u001b[0m: A target array with shape (20974, 21) was passed for an output of shape (None, 6) while using as loss `categorical_crossentropy`. This loss expects targets to have the same shape as the output."
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.layers import Embedding, SpatialDropout1D, LSTM, Dense\n",
    "#import tensorsflow_addons as tfa\n",
    "import keras\n",
    "import tensorflow\n",
    "from keras.layers import Embedding, SpatialDropout1D, LSTM, Dense\n",
    "#import tensorsflow_addons as tfa\n",
    "vocab_size=50000\n",
    "embedding_dim=64\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    # Add an Embedding layer expecting input vocab of size 5000, and output embedding dimension of size 64 we set at the top\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim)),\n",
    "#    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "    # use ReLU in place of tanh function since they are very good alternatives of each other.\n",
    "    tf.keras.layers.Dense(embedding_dim, activation='relu'),\n",
    "    # Add a Dense layer with 6 units and softmax activation.\n",
    "    # When we have multiple outputs, softmax convert outputs layers into a probability distribution.\n",
    "    tf.keras.layers.Dense(6, activation='softmax')\n",
    "])\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "history = model.fit(X_tr, Y_tr, epochs=epochs, batch_size=batch_size,validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04607329650624178\n"
     ]
    }
   ],
   "source": [
    "y_pred= model.predict(X_te)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "y_test = test_data['#classes_id']\n",
    "Y_te = pd.get_dummies(y_test).values\n",
    "\n",
    "Y_te = np.argmax(Y_te, axis=1)\n",
    "print(f1_score(Y_te, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
