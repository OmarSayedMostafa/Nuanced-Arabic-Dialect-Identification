{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from utilities import remove_empty_tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: (20974, 8) 20974\n",
      "Test set: (4997, 8) 4997\n"
     ]
    }
   ],
   "source": [
    "train_data_path = 'cleaned_data/cleaned_train_data_for_subtask1.csv'\n",
    "test_data_path = 'cleaned_data/cleaned_test_data_for_subtask1.csv'\n",
    "#read files.\n",
    "train_data = pd.read_csv(train_data_path)\n",
    "test_data = pd.read_csv(test_data_path)\n",
    "\n",
    "print(\"Train set:\"% train_data.columns, train_data.shape, len(train_data)) \n",
    "print(\"Test set:\"% test_data.columns, test_data.shape, len(test_data)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#1_tweetid</th>\n",
       "      <th>#2_tweet</th>\n",
       "      <th>#3_country_label</th>\n",
       "      <th>#2_tweet_clean_V0</th>\n",
       "      <th>#2_tweet_clean_V1</th>\n",
       "      <th>#2_tweet_clean_V2</th>\n",
       "      <th>#2_tweet_clean_V3</th>\n",
       "      <th>#classes_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_0</td>\n",
       "      <td>حاجة حلوة اكيد</td>\n",
       "      <td>Egypt</td>\n",
       "      <td>حاجة حلوة اكيد</td>\n",
       "      <td>حاجه حلوه اكيد</td>\n",
       "      <td>حاجه حلوه اكيد</td>\n",
       "      <td>حاجه حلوه اكيد</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_1</td>\n",
       "      <td>عم بشتغلوا للشعب الاميركي اما نحن يكذبوا ويغشو...</td>\n",
       "      <td>Iraq</td>\n",
       "      <td>عم بشتغلوا للشعب الاميركي اما نحن يكذبوا ويغشو...</td>\n",
       "      <td>عم بشتغلوا لشعب الاميركي اما نحن يكذبوا ويغشوا...</td>\n",
       "      <td>عم بشتغلوا لشعب الاميركي يكذبوا ويغشوا ويسرقوا...</td>\n",
       "      <td>عم بشتغلوا لشعب الاميركي يكذبوا ويغشوا ويسرقوا...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_2</td>\n",
       "      <td>ابشر طال عمرك</td>\n",
       "      <td>Saudi_Arabia</td>\n",
       "      <td>ابشر طال عمرك</td>\n",
       "      <td>ابشر طال عمرك</td>\n",
       "      <td>ابشر طال عمرك</td>\n",
       "      <td>ابشر طال عمرك</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_3</td>\n",
       "      <td>منطق 2017: أنا والغريب علي إبن عمي وأنا والغري...</td>\n",
       "      <td>Mauritania</td>\n",
       "      <td>منطق  أنا والغريب علي إبن عمي وأنا والغريب وإب...</td>\n",
       "      <td>منطق انا والغريب علي ابن عمي وانا والغريب وابن...</td>\n",
       "      <td>منطق والغريب ابن عمي وانا والغريب وابن عمي اخو...</td>\n",
       "      <td>منطق والغريب ابن عمي وانا وابن اخويا قطع العلا...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_4</td>\n",
       "      <td>شهرين وتروح والباقي غير صيف ملينا</td>\n",
       "      <td>Algeria</td>\n",
       "      <td>شهرين وتروح والباقي غير صيف ملينا</td>\n",
       "      <td>شهرين وتروح والباقي غير صيف ملينا</td>\n",
       "      <td>شهرين وتروح والباقي صيف ملينا</td>\n",
       "      <td>شهرين وتروح والباقي صيف ملينا</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  #1_tweetid                                           #2_tweet  \\\n",
       "0    TRAIN_0                                     حاجة حلوة اكيد   \n",
       "1    TRAIN_1  عم بشتغلوا للشعب الاميركي اما نحن يكذبوا ويغشو...   \n",
       "2    TRAIN_2                                      ابشر طال عمرك   \n",
       "3    TRAIN_3  منطق 2017: أنا والغريب علي إبن عمي وأنا والغري...   \n",
       "4    TRAIN_4                  شهرين وتروح والباقي غير صيف ملينا   \n",
       "\n",
       "  #3_country_label                                  #2_tweet_clean_V0  \\\n",
       "0            Egypt                                     حاجة حلوة اكيد   \n",
       "1             Iraq  عم بشتغلوا للشعب الاميركي اما نحن يكذبوا ويغشو...   \n",
       "2     Saudi_Arabia                                      ابشر طال عمرك   \n",
       "3       Mauritania  منطق  أنا والغريب علي إبن عمي وأنا والغريب وإب...   \n",
       "4          Algeria                  شهرين وتروح والباقي غير صيف ملينا   \n",
       "\n",
       "                                   #2_tweet_clean_V1  \\\n",
       "0                                     حاجه حلوه اكيد   \n",
       "1  عم بشتغلوا لشعب الاميركي اما نحن يكذبوا ويغشوا...   \n",
       "2                                      ابشر طال عمرك   \n",
       "3  منطق انا والغريب علي ابن عمي وانا والغريب وابن...   \n",
       "4                  شهرين وتروح والباقي غير صيف ملينا   \n",
       "\n",
       "                                   #2_tweet_clean_V2  \\\n",
       "0                                     حاجه حلوه اكيد   \n",
       "1  عم بشتغلوا لشعب الاميركي يكذبوا ويغشوا ويسرقوا...   \n",
       "2                                      ابشر طال عمرك   \n",
       "3  منطق والغريب ابن عمي وانا والغريب وابن عمي اخو...   \n",
       "4                      شهرين وتروح والباقي صيف ملينا   \n",
       "\n",
       "                                   #2_tweet_clean_V3  #classes_id  \n",
       "0                                     حاجه حلوه اكيد            0  \n",
       "1  عم بشتغلوا لشعب الاميركي يكذبوا ويغشوا ويسرقوا...            1  \n",
       "2                                      ابشر طال عمرك            2  \n",
       "3  منطق والغريب ابن عمي وانا وابن اخويا قطع العلا...            3  \n",
       "4                      شهرين وتروح والباقي صيف ملينا            4  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = remove_empty_tweets(train_data, \"#2_tweet_clean_V1\")\n",
    "test = remove_empty_tweets(test_data, \"#2_tweet_clean_V1\")\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare train and test data.\n",
    "X_train = train_data['#2_tweet_clean_V1'].tolist()\n",
    "y_train = train_data['#classes_id'].tolist()\n",
    "X_test = test_data['#2_tweet_clean_V1'].tolist()\n",
    "y_test = test_data['#classes_id'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_targets(y_train, y_test):\n",
    "    le = LabelEncoder()\n",
    "    le.fit(y_train)\n",
    "    y_train_enc = le.transform(y_train)\n",
    "    y_test_enc = le.transform(y_test)\n",
    "    return y_train_enc, y_test_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18876 2098\n"
     ]
    }
   ],
   "source": [
    "x_train, x_valid, y_train, y_valid = model_selection.train_test_split(X_train,y_train,test_size=0.1, random_state=42)\n",
    "\n",
    "print(len(x_train),len(x_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import copy\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import re\n",
    "# cross validation and metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.optim.optimizer import Optimizer\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from multiprocessing import  Pool\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 300 # how big is each word vector\n",
    "max_features = 120000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 750 # max number of words in a question to use\n",
    "batch_size = 512 # how many samples to process at once\n",
    "n_epochs = 5 # how many times to iterate over all samples\n",
    "n_splits = 5 # Number of K-fold Splits\n",
    "SEED = 10\n",
    "debug = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Text(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CNN_Text, self).__init__()\n",
    "        filter_sizes = [1,2,3,5]\n",
    "        num_filters = 36\n",
    "        n_classes = len(le.classes_)\n",
    "        self.embedding = nn.Embedding(max_features, embed_size)\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.convs1 = nn.ModuleList([nn.Conv2d(1, num_filters, (K, embed_size)) for K in filter_sizes])\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc1 = nn.Linear(len(filter_sizes)*num_filters, n_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  \n",
    "        x = x.unsqueeze(1)  \n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1] \n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  \n",
    "        x = torch.cat(x, 1)\n",
    "        x = self.dropout(x)  \n",
    "        logit = self.fc1(x) \n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words = max_features)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(x_train)\n",
    "train_padded = pad_sequences(train_sequences, maxlen=maxlen)\n",
    "\n",
    "validation_sequences = tokenizer.texts_to_sequences(x_valid)\n",
    "validation_padded = pad_sequences(validation_sequences)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(y_train)\n",
    "y_train_enc = le.transform(y_train)\n",
    "y_valid_enc = le.transform(y_valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maryam\\anaconda3\\envs\\envName\\lib\\site-packages\\ipykernel_launcher.py:48: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6 \t loss=1350.6006 \t val_loss=1071.2194  \t val_acc=0.2421  \t time=229.19s\n",
      "Epoch 2/6 \t loss=1236.6215 \t val_loss=1049.5367  \t val_acc=0.2641  \t time=192.59s\n",
      "Epoch 3/6 \t loss=1161.3348 \t val_loss=1031.4064  \t val_acc=0.2912  \t time=199.62s\n",
      "Epoch 4/6 \t loss=1084.4691 \t val_loss=1024.5702  \t val_acc=0.2989  \t time=208.92s\n",
      "Epoch 5/6 \t loss=1008.8950 \t val_loss=1021.4107  \t val_acc=0.2984  \t time=206.87s\n",
      "Epoch 6/6 \t loss=937.3877 \t val_loss=1022.5667  \t val_acc=0.3031  \t time=202.99s\n"
     ]
    }
   ],
   "source": [
    "import torch.utils.data\n",
    "n_epochs = 6\n",
    "model = CNN_Text()\n",
    "loss_fn = nn.CrossEntropyLoss(reduction='sum')\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\n",
    "\n",
    "# Load train and test in CUDA Memory\n",
    "x_train = torch.tensor(train_padded, dtype=torch.long)\n",
    "y_train = torch.tensor(y_train_enc, dtype=torch.long)\n",
    "x_cv = torch.tensor(validation_padded, dtype=torch.long)\n",
    "y_cv = torch.tensor(y_valid_enc, dtype=torch.long)\n",
    "\n",
    "# Create Torch datasets\n",
    "train = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "valid = torch.utils.data.TensorDataset(x_cv, y_cv)\n",
    "\n",
    "# Create Data Loaders\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    start_time = time.time()\n",
    "    # Set model to train configuration\n",
    "    model.train()\n",
    "    avg_loss = 0.  \n",
    "    for i, (x_batch, y_batch) in enumerate(train_loader):\n",
    "        # Predict/Forward Pass\n",
    "        y_pred = model(x_batch)\n",
    "        # Compute loss\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        avg_loss += loss.item() / len(train_loader)\n",
    "    \n",
    "    # Set model to validation configuration -Doesn't get trained here\n",
    "    model.eval()        \n",
    "    avg_val_loss = 0.\n",
    "    val_preds = np.zeros((len(x_cv),len(le.classes_)))\n",
    "    \n",
    "    for i, (x_batch, y_batch) in enumerate(valid_loader):\n",
    "        y_pred = model(x_batch).detach()\n",
    "        avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)\n",
    "        # keep/store predictions\n",
    "        val_preds[i * batch_size:(i+1) * batch_size] =F.softmax(y_pred).cpu().numpy()\n",
    "    \n",
    "    # Check Accuracy\n",
    "    val_accuracy = sum(val_preds.argmax(axis=1)==y_valid_enc)/len(y_valid_enc)\n",
    "    train_loss.append(avg_loss)\n",
    "    valid_loss.append(avg_val_loss)\n",
    "    elapsed_time = time.time() - start_time \n",
    "    print('Epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f}  \\t val_acc={:.4f}  \\t time={:.2f}s'.format(\n",
    "                epoch + 1, n_epochs, avg_loss, avg_val_loss, val_accuracy, elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
